"""
invoke-lambda â€“ ä¾ä½¿ç”¨è€…è¼¸å…¥èª¿ç”¨ Bedrock Agentï¼Œä¸¦å°‡å›æ‡‰æ•´ç†æˆ
ç¬¦åˆå‰ç«¯ (Streamlit) ä½¿ç”¨çš„ HTML ç‰‡æ®µã€‚

â€¢ å…ˆä»¥ session_id å€åˆ†åŒä¸€ä½ä½¿ç”¨è€…ä¸€æ¬¡ã€Œå®Œæ•´åˆ†ææµç¨‹ã€ã€‚
â€¢ è§£æ Agent traceï¼Œå–å‡º:
    1) ä¸»å›ç­”æ–‡å­— (chunk)
    2) SQL queryï¼ˆå¦‚æœ‰ ACTION_GROUPï¼‰
    3) KB ä¾†æºåƒè€ƒé€£çµï¼ˆS3 URIï¼‰
â€¢ å†å‘¼å«ç¬¬äºŒå€‹ FMï¼ˆClaude 3 Sonnetï¼‰æŠŠä¸»å›ç­”â†’HTML+JSON æ ¼å¼åŒ–ã€‚
â€¢ é€é `parse_json_from_text()` ä¿è­‰è¼¸å‡ºä¸€å®šèƒ½è¢« json.loads()ã€‚
"""

from __future__ import annotations

import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple
from urllib.parse import unquote
import io
import time
import random
import uuid
import base64

from connections import Connections
from utils import (
    output_format, 
    evaluation_prompt_en, 
    parse_json_from_text, 
    combine_html_from_json,
    get_heading_prefix
)

# ============ Logger ============
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# ============ ç’°å¢ƒè®Šæ•¸ ============
AGENT_ID: str = os.environ["AGENT_ID"]
REGION_NAME: str = os.environ["REGION_NAME"]

SUPPLY_INFO = "" # "ç›®å‰æœªå……åˆ†è³‡æ–™ï¼Œå°‡ä½¿ç”¨æ—¢æœ‰çŸ¥è­˜åˆ†æ..."

logger.info("Bedrock Agent ID: %s", AGENT_ID)

# ============ AWS é€£ç·š ============
agent_client = Connections.agent_client
agent_runtime_client = Connections.agent_runtime_client
s3_resource = Connections.s3_resource
s3_client_fbmapping = Connections.s3_client_fbmapping

# ---------------------------------------------------------------------
# Agent helpers
# ---------------------------------------------------------------------
def get_highest_agent_version_alias_id(
    list_resp: Dict[str, Any]
) -> str | None:
    """
    å–å¾—ç›®å‰ Agent æ‰€æœ‰åˆ¥å (Aliases) ä¸­ç‰ˆæœ¬è™Ÿæœ€å¤§çš„é‚£ä¸€å€‹ã€‚

    Parameters
    ----------
    list_resp : dict
        `list_agent_aliases` çš„å›å‚³çµæœã€‚

    Returns
    -------
    str | None
        è‹¥æ‰¾ä¸åˆ°å¯ç”¨ aliasï¼Œå›å‚³ Noneã€‚
    """
    highest = -1
    best_alias_id = None
    for alias in list_resp.get("agentAliasSummaries", []):
        rcfg = alias.get("routingConfiguration")
        if rcfg:
            ver = rcfg[0]["agentVersion"]
            if ver.isdigit() and int(ver) > highest:
                highest = int(ver)
                best_alias_id = alias["agentAliasId"]
    return best_alias_id


def build_prompt(user_input: str, topic: str, company_info: Dict[str, str]) -> str:
    """
    å°‡å…¬å¸åŸºæœ¬è³‡æ–™ + Topic + ä½¿ç”¨è€…å•é¡Œ çµ„æˆ Agent Promptã€‚

    ä¾‹ï¼š
    ã€Œå“ç‰Œåç¨±ï¼šå°ç£å•¤é…’ï¼Œå“ç‰Œæ‰€å±¬ç”¢æ¥­ï¼šå•¤é…’ ...ã€‚
      è«‹ä»¥ã€å¸‚å ´æ¦‚æ³èˆ‡è¶¨å‹¢ã€çš„è§’åº¦â€¦ã€
    """
    base = "ï¼Œ".join(f"{k}ï¼š{v}" for k, v in company_info.items() if v.strip())
    return f"è«‹ç”¨searchinternetä¸Šç¶²æœå°‹ã€querygluetableæŠ“è³‡æ–™åŠåˆ©ç”¨knwoledge baseæŠ“è³‡æ–™ã€‚\n\n{base}ã€‚\n\né‡å°{topic}é€²è¡Œåˆ†æï¼Œå°ˆæ¥­åœ°å›ç­”ä½¿ç”¨è€…å•é¡Œï¼š{user_input}"


def invoke_agent(
    user_input: str, session_id: str, topic: str, company_info: Dict[str, str]
):
    """
    å‘¼å« Bedrock Agent ä¸¦å›å‚³ streaming response (generator ç‰©ä»¶)ã€‚
    """
    alias_id = get_highest_agent_version_alias_id(
        agent_client.list_agent_aliases(agentId=AGENT_ID)
    )
    if not alias_id:
        raise RuntimeError("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„ Agent Alias")

    prompt = build_prompt(user_input, topic, company_info)
    return agent_runtime_client.invoke_agent(
        agentId=AGENT_ID,
        agentAliasId=alias_id,
        sessionId=session_id,
        enableTrace=True,
        inputText=prompt,
    )

# ---------------------------------------------------------------------
# Trace / å›æ‡‰è§£æ
# ---------------------------------------------------------------------
def get_agent_response(streaming_resp: Dict[str, Any], topic: str = "") -> Tuple[str, List[str], List[dict[str, str]]]:
    """
    è§£æ Agent Streaming Response:
    - å›ç­”æ–‡å­— chunk
    - æ‰€æœ‰ä¾†æºé€£çµï¼ˆKB + Perplexityï¼‰
    - åœ–è¡¨æ•¸æ“šï¼š[(title_text, img_html), ...]
    
    æ–°å¢åƒæ•¸:
    - topic: åˆ†æä¸»é¡Œï¼Œç”¨æ–¼æ§åˆ¶æ˜¯å¦åŒ…å«åœ–è¡¨ä¾†æº
    """
    logger.info(f"get_agent_response ... topic: {topic}")
    if "completion" not in streaming_resp:
        raise ValueError("Invalid response: missing 'completion' field")

    traces: List[dict] = []
    chunk_text = []

    for event in streaming_resp["completion"]:
        if "trace" in event:
            logger.info("trace: %s", event["trace"])
            traces.append(event["trace"])
        elif "chunk" in event:
            chunk_text.append(event["chunk"]["bytes"].decode("utf-8", "ignore"))

    full_text = "".join(chunk_text)

    # æ”¶é›†æ‰€æœ‰ä¾†æºï¼ˆKnowledge Base + Perplexityï¼‰
    sources: List[str] = []

    try:
        kb_sources = extract_source_list_from_kb(traces)
        sources.extend(kb_sources)
        if kb_sources:
            logger.info("âœ“ Extracted %d KB references", len(kb_sources))
    except Exception as err:
        logger.warning("âš ï¸ Extract KB refs failed: %s", err)

    try:
        web_sources = extract_source_list_from_perplexity(traces)
        sources.extend(web_sources)
        if web_sources:
            logger.info("âœ“ Extracted %d Perplexity sources", len(web_sources))
    except Exception as err:
        logger.warning("âš ï¸ Extract Perplexity refs failed: %s", err)

    # æ ¹æ“šä¸»é¡Œæ±ºå®šæ˜¯å¦å°‡åœ–è¡¨ä¾†æºåŠ å…¥ sources
    try:
        txt2figure_result = extract_txt2figure_result_from_traces(traces)
        
        # åªæœ‰åœ¨"å¸‚å ´æ¦‚æ³èˆ‡è¶¨å‹¢"ä¸»é¡Œæ™‚æ‰å°‡åœ–è¡¨ä¾†æºåŠ å…¥ sources
        if topic == "å¸‚å ´æ¦‚æ³èˆ‡è¶¨å‹¢":
            # å¾åœ–è¡¨æ•¸æ“šä¸­æå– title_text ä½œç‚ºä¾†æº
            chart_sources = [chart["title_text"] for chart in txt2figure_result]
            sources.extend(chart_sources)
            logger.info("âœ“ Extracted %d Athena reference (included in sources)", len(chart_sources))
        else:
            logger.info("âœ“ Extracted %d Athena reference (excluded from sources due to topic: %s)", 
                    len(txt2figure_result), topic)
            
    except Exception as err:
        logger.warning("âš ï¸ Extract Athena refs failed: %s", err)
        txt2figure_result = []

    return full_text, sources, txt2figure_result

# ---------------------------------------------------------------------
# Athena-Txt2Figure ä¾†æºè™•ç†
# ---------------------------------------------------------------------
def extract_txt2figure_result_from_traces(traces: List[dict]) -> List[Dict[str, Any]]:
    """
    è‹¥ trace ä¸­åŒ…å« ACTION_GROUPï¼Œæ“·å– Figure-HTML ç‰‡æ®µã€‚
    ç¢ºä¿ img_static å­—æ®µå¾ base64 å­—ç¬¦ä¸²é‚„åŸç‚º bytes
    ä¸»å‡½æ•¸ï¼šå¾tracesä¸­æå–txt2figureçµæœï¼Œè·è²¬ï¼šå”èª¿æ•´å€‹æå–æµç¨‹
    """
    try:
        for trace in traces:
            logger.info("extract_figure_from_traces - trace: %s", trace)
            
            vanna_result = _extract_vanna_result_from_trace(trace)
            if vanna_result:
                processed_result = _process_vanna_result(vanna_result)
                return _add_title_suffix(processed_result)
        
        return []
        
    except Exception as err:
        logger.warning("âš ï¸ Extract Athena refs failed: %s", err)
        return []

def _extract_vanna_result_from_trace(trace: dict) -> list | None:
    """
    å¾ traces ä¸­æå– vanna_result_by_titleï¼Œè½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼ã€‚
    æ”¯æ´æ–°çš„å¤šSQLçµæ§‹ã€‚
    """
    obs = (
        trace.get("trace", {})
            .get("orchestrationTrace", {})
            .get("observation", {})
    )

    # åƒ…è™•ç† ACTION_GROUP
    if obs.get("type") != "ACTION_GROUP":
        return None

    ag_out = obs.get("actionGroupInvocationOutput", {})

    # å„ªå…ˆæª¢æŸ¥ sessionAttributes
    session_attrs = ag_out.get("sessionAttributes", {})
    if session_attrs and "vanna_result_by_title" in session_attrs:
        vanna_by_title = session_attrs["vanna_result_by_title"]
        if isinstance(vanna_by_title, dict):
            # å°‡å­—å…¸å€¼è½‰æ›ç‚ºåˆ—è¡¨ï¼Œä¸¦è™•ç†å¯èƒ½çš„å¤šå€‹çµæœ
            result_list = []
            for title, result_data in vanna_by_title.items():
                if isinstance(result_data, list):
                    # å¦‚æœä¸€å€‹ title å°æ‡‰å¤šå€‹çµæœï¼ˆå¤šå€‹ SQLï¼‰
                    result_list.extend(result_data)
                else:
                    # å–®å€‹çµæœ
                    result_list.append(result_data)
            return result_list

    # å›æº¯ text æ¬„ä½
    text_blob = ag_out.get("text", "")
    if not text_blob:
        return None

    try:
        payload = json.loads(text_blob) if isinstance(text_blob, str) else text_blob
    except json.JSONDecodeError:
        return None

    if isinstance(payload, dict) and "vanna_result_by_title" in payload:
        vanna_by_title = payload["vanna_result_by_title"]
        if isinstance(vanna_by_title, dict):
            # å°‡å­—å…¸å€¼è½‰æ›ç‚ºåˆ—è¡¨ï¼Œä¸¦è™•ç†å¯èƒ½çš„å¤šå€‹çµæœ
            result_list = []
            for title, result_data in vanna_by_title.items():
                if isinstance(result_data, list):
                    # å¦‚æœä¸€å€‹ title å°æ‡‰å¤šå€‹çµæœï¼ˆå¤šå€‹ SQLï¼‰
                    result_list.extend(result_data)
                else:
                    # å–®å€‹çµæœ
                    result_list.append(result_data)
            return result_list

    return None

def _process_vanna_result(vanna_result: list) -> List[dict]:
    """
    è·è²¬ï¼šè™•ç†vanna_resultï¼Œå°‡base64å­—ç¬¦ä¸²é‚„åŸç‚ºbytes
    ç­‰åŒæ–¼åŸå§‹çš„ restore_bytes_from_base64 å‡½æ•¸åŠŸèƒ½
    """
    def restore_bytes_from_base64(obj):
        """
        å°‡ base64 å­—ç¬¦ä¸²é‚„åŸç‚º bytesï¼Œéæ­¸è™•ç†åµŒå¥—çµæ§‹
        """
        if isinstance(obj, dict):
            result = {}
            for k, v in obj.items():
                if k == "img_static" and isinstance(v, str) and v:
                    logger.info("æ­£åœ¨ä¸‹è¼‰ S3 åœ–ç‰‡: %s", v)
                    # v å·²æ˜¯ S3 ä½ç½®ï¼Œä¾‹ï¼š
                    #   https://my-bucket.s3.amazonaws.com/vanna/abcd.png
                    try:
                        if v.startswith("s3://"):
                            # s3://bucket/key  â†’  bucket, key
                            bucket, key = v.replace("s3://", "").split("/", 1)
                            response = s3_client_fbmapping.get_object(Bucket=bucket, Key=key)
                            data = response["Body"].read()
                        else:  # HTTPS å½¢å¼
                            # è½‰å› bucketã€key å¾Œç›´æ¥ç”¨ boto3 ä¸‹è¼‰ï¼ˆåŸŸååŒ bucketï¼‰
                            m = re.match(r"https://([^.]*)\.s3.*?/(.+)", v)
                            bucket, key = m.group(1), m.group(2)
                            response = s3_client_fbmapping.get_object(Bucket=bucket, Key=key)
                            data = response["Body"].read()
                        result[k] = {
                            "bytes": data,  # Word ç”¨
                            "b64":  base64.b64encode(data).decode()  # èµ° JSON ç”¨
                        }
                    except Exception as e:
                        logger.warning(f"ç„¡æ³•ä¸‹è¼‰ S3 åœ–ç‰‡: {e}")
                        result[k] = None
                else:
                    result[k] = restore_bytes_from_base64(v)
            return result
        elif isinstance(obj, list):
            return [restore_bytes_from_base64(item) for item in obj]
        else:
            return obj
    
    # é‚„åŸ base64 ç‚º bytes
    processed_result = restore_bytes_from_base64(vanna_result)
    logger.info("æˆåŠŸé‚„åŸ base64 æ•¸æ“šç‚º bytes")
    
    # ç›´æ¥ä½¿ç”¨åŸå§‹HTMLï¼Œä¸é€²è¡Œè½‰æ›
    logger.info("ä¿æŒåŸå§‹ Plotly HTML")
    
    return processed_result

def _add_title_suffix(result: List[dict], suffix: str = "(ç™¼ç¥¨æ•¸æ“š)") -> List[dict]:
    """
    ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡è³‡è¨Šç”¨æ–¼åœ–è¡¨å®šä½
    """
    for item in result:
        if isinstance(item, dict):
            orig_title = item.get("title_text") or ""
            question = item.get("question", "")
            
            # ä¿ç•™åŸå§‹å•é¡Œä½œç‚ºcontextï¼Œç”¨æ–¼å¾ŒçºŒçš„åœ–è¡¨å®šä½
            if question and "context" not in item:
                item["context"] = question
            
            if not orig_title:
                if question:
                    simplified_title = question[:30] + "..." if len(question) > 30 else question
                    item["title_text"] = f"{simplified_title}{suffix}"
                else:
                    item["title_text"] = suffix
            elif not orig_title.endswith(suffix):
                item["title_text"] = f"{orig_title}{suffix}"

    return result

# ---------------------------------------------------------------------
# Knowledge-Base ä¾†æºè™•ç†
# ---------------------------------------------------------------------
def extract_source_list_from_kb(traces: List[dict]) -> List[str]:
    """
    å¾ KB Trace èƒå– S3 URI listã€‚
    """
    for t in traces:
        obs = (
            t.get("trace", {})
            .get("orchestrationTrace", {})
            .get("observation", {})
        )
        kb_out = obs.get("knowledgeBaseLookupOutput")
        if kb_out:
            return [r["location"]["s3Location"]["uri"] for r in kb_out["retrievedReferences"]]
    return []


def clean_and_dedup_uris(uris: list[str]) -> list[str]:
    seen = set()
    deduped = []
    for u in uris:
        if not u or not isinstance(u, str):
            continue
        norm = unquote(u.strip().lower())
        if norm not in seen:
            seen.add(norm)
            deduped.append(u)
    return deduped


def source_link(uris: list[str]) -> str:
    uris = clean_and_dedup_uris(uris)

    if not uris:
        return "<p><em>ç›®å‰ç„¡å¯ç”¨çš„ä¾†æºè³‡æ–™ã€‚</em></p>"

    links = []
    source_unknow_idx = 1

    for item in uris:
        try:
            item = item.strip() if item else ""
            if not item:
                continue

            # åˆ¤æ–·æ˜¯å¦ç‚º agent question
            if "(ç™¼ç¥¨æ•¸æ“š)" in item:
                # safe_text = html.escape(item)
                safe_text = item
                links.append(f"<li><span class='question-text'>{safe_text}</span></li>")
                continue

            filename = item.rsplit("/", 1)[-1]
            filename = unquote(filename)
            filename = re.sub(r"[_\-]+", " ", filename).strip()
            # safe_item = html.escape(item)
            safe_item = item

            # åˆ¤æ–· s3 é‚„æ˜¯ http
            if item.lower().startswith("s3://"):
                label = f"{filename}ï¼ˆä¾†æºæª”æ¡ˆï¼‰" if filename else "ä¾†æºæª”æ¡ˆ"
            else:
                label = filename or f"æœªå‘½åä¾†æº {source_unknow_idx}"
                if "æœªå‘½åä¾†æº" in label:
                    source_unknow_idx += 1

            # safe_label = html.escape(label)
            safe_label = label
            links.append(f"<li>. <a href='{safe_item}' target='_blank'>{safe_label}</a></li>")

        except Exception as e:
            logger.warning("Failed to parse URI %s: %s", item, e)
            # links.append(f"<li>. <a href='{html.escape(item)}' target='_blank'>æœªçŸ¥ä¾†æº</a></li>")
            links.append(f"<li>. <a href='{item}' target='_blank'>æœªçŸ¥ä¾†æº</a></li>")

    return "<ul>" + "\n".join(links) + "</ul>"

# ---------------------------------------------------------------------
# Web-Search ä¾†æºè™•ç†
# ---------------------------------------------------------------------
def extract_source_list_from_perplexity(traces: List[dict]) -> List[str]:
    """
    å¾ Perplexity Trace ä¸­æ“·å–å¤–éƒ¨ä¾†æºé€£çµã€‚
    å°‹æ‰¾ trace â†’ orchestrationTrace â†’ observation â†’ actionGroupInvocationOutput.textï¼Œ
    è‹¥è©²æ®µè½ç‚º JSON æ ¼å¼ä¸”å«æœ‰ 'response.sources'ï¼Œå³è¦–ç‚º Perplexity çµæœã€‚
    """
    sources = []

    for t in traces:
        obs = (
            t.get("trace", {})
             .get("orchestrationTrace", {})
             .get("observation", {})
        )
        text = obs.get("actionGroupInvocationOutput", {}).get("text", "")

        try:
            payload = json.loads(text)
            if isinstance(payload, dict):
                source_list = payload.get("response", {}).get("sources", [])
                if isinstance(source_list, list):
                    sources.extend(source_list)
        except json.JSONDecodeError:
            continue  # è·³éé JSON æ ¼å¼

    return sources

# ---------------------------------------------------------------------
# Claude Sonnet â€“ ç”¢ç”Ÿ HTML/JSON å ±å‘Š
# ---------------------------------------------------------------------
import threading
import concurrent.futures
from typing import List, Tuple

def create_chart_placeholder(chart_id: str) -> str:
    """å‰µå»ºåœ–è¡¨ä½”ä½ç¬¦ï¼Œé¿å…JSONåºåˆ—åŒ–å•é¡Œ"""
    return f"""
    <div class="chart-body" id="chart-body-{chart_id}" style="
        padding: 20px;
        min-height: 400px;
    ">
        <div id="plotly-placeholder-{chart_id}" style="
            display: flex;
            align-items: center;
            justify-content: center;
            height: 350px;
            color: #666;
            font-size: 16px;
        ">
            ğŸ“Š åœ–è¡¨åŠ è¼‰ä¸­...
        </div>
    </div>
    """

def build_output_format(raw_analysis: str, topic: str, txt2figure_results: List[dict[str, Any]]) -> Dict[str, Any]:
    """å¢å¼·ç‰ˆçš„build_output_formatï¼Œçµ±ä¸€HTMLå’ŒWordåœ–è¡¨æ’å…¥é‚è¼¯"""
    topic_data = output_format.get(topic)
    if not topic_data:
        raise ValueError(f"Unsupported topic: {topic}")

    result = {}
    chart_data = {}
    word_chart_data = {}  # æ–°å¢ï¼šWordå°ˆç”¨åœ–è¡¨æ•¸æ“š
    subtopics = topic_data.get("subtopics", [])

    # æ·»åŠ å¤§æ¨™é¡Œ - ä½¿ç”¨ç‰¹æ®Šå‰ç¶´ç¢ºä¿åœ¨æœ€å‰é¢
    main_title = topic_data.get("title", topic)
    result["_000_main_title"] = f"<h1>{main_title}</h1>"

    # æ”¶é›†æ‰€æœ‰ä»»å‹™ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰
    all_tasks = []
    for subtopic_idx, subtopic in enumerate(subtopics):
        subtopic_title = subtopic["title"]
        subsubtopics = subtopic.get("subsubtopics", [])
        
        if not subsubtopics:
            all_tasks.append(("subtopic", subtopic_idx, subtopic_title, None, None))
        else:
            for subsubtopic_idx, subsub in enumerate(subsubtopics):
                if isinstance(subsub, dict) and 'title' in subsub:
                    subsub_title = subsub["title"]
                else:
                    subsub_title = subsub
                all_tasks.append(("subsubtopic", subtopic_idx, subtopic_title, subsubtopic_idx, subsub_title))

    # å…§å®¹ç”Ÿæˆè™•ç†ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰
    if all_tasks:
        logger.info(f"é–‹å§‹è™•ç† {len(all_tasks)} å€‹ä»»å‹™ï¼ˆå¢å¼·å¯é æ€§ç‰ˆæœ¬ï¼‰")
        
        # ç¬¬ä¸€è¼ªï¼šæ­£å¸¸ä¸¦ç™¼è™•ç†ï¼ˆé™ä½ä¸¦ç™¼æ•¸ä»¥æ¸›å°‘é™æµï¼‰
        optimal_workers = min(16, len(all_tasks), 20)  # é™ä½ä¸¦ç™¼æ•¸
        logger.info(f"ä½¿ç”¨ {optimal_workers} å€‹ç·šç¨‹ä¸¦ç™¼è™•ç†ä»»å‹™")
        
        completed_results = []
        failed_tasks = []
        
        def call_model_unified_enhanced(task_info):
            try:
                task_type, subtopic_idx, subtopic_title, subsubtopic_idx, subsubtopic_title = task_info
                
                # ç”Ÿæˆå”¯ä¸€task_id
                if task_type == "subtopic":
                    task_id = f"main_{subtopic_title}_{subtopic_idx}"
                else:
                    task_id = f"main_{subsubtopic_title}_{subtopic_idx}_{subsubtopic_idx}"
                
                # åŸºæ–¼ä»»å‹™ç´¢å¼•éŒ¯é–‹åŸ·è¡Œæ™‚é–“
                if task_type == "subtopic":
                    global_task_idx = subtopic_idx
                else:
                    global_task_idx = subtopic_idx * 10 + subsubtopic_idx
                
                delay = (global_task_idx % 8) * 0.5
                time.sleep(delay)
                
                return call_model_unified(task_info, raw_analysis, task_id)
                
            except Exception as e:
                logger.error(f"ä»»å‹™åŸ·è¡Œå¤±æ•—: {task_info}, éŒ¯èª¤: {e}")
                return None

        # ç¬¬ä¸€è¼ªåŸ·è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=optimal_workers) as executor:
            futures = [executor.submit(call_model_unified_enhanced, task_info) for task_info in all_tasks]
            
            for i, future in enumerate(concurrent.futures.as_completed(futures)):
                try:
                    result_tuple = future.result()
                    if result_tuple and result_tuple[5]:  # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå…§å®¹
                        completed_results.append(result_tuple)
                        task_type, _, subtopic_title, _, target_title, _ = result_tuple
                        display_target = target_title or f"(å­æ¨™é¡Œ: {subtopic_title})"
                        logger.info(f"å®Œæˆé€²åº¦: {i+1}/{len(futures)} - [{task_type}] -> {display_target}")
                    else:
                        # è¨˜éŒ„å¤±æ•—çš„ä»»å‹™
                        failed_tasks.append(all_tasks[i])
                        logger.warning(f"ä»»å‹™å¤±æ•—ï¼Œå°‡é‡è©¦: {all_tasks[i]}")
                except Exception as e:
                    failed_tasks.append(all_tasks[i])
                    logger.error(f"ä»»å‹™åŸ·è¡Œç•°å¸¸: {e}")

        # ç¬¬äºŒè¼ªï¼šé‡è©¦å¤±æ•—çš„ä»»å‹™
        if failed_tasks:
            logger.warning(f"ç¬¬ä¸€è¼ªå®Œæˆ {len(completed_results)}/{len(all_tasks)} å€‹ä»»å‹™ï¼Œé–‹å§‹é‡è©¦ {len(failed_tasks)} å€‹å¤±æ•—ä»»å‹™")
            retry_results = retry_failed_tasks(failed_tasks, raw_analysis)
            completed_results.extend(retry_results)
            logger.info(f"é‡è©¦å®Œæˆï¼Œç¸½è¨ˆæˆåŠŸ: {len(completed_results)}/{len(all_tasks)} å€‹ä»»å‹™")

        # ç¬¬ä¸‰è¼ªï¼šç‚ºä»ç„¶å¤±æ•—çš„ä»»å‹™ç”Ÿæˆfallbackå…§å®¹
        successful_titles = {result[4] for result in completed_results}  # target_title
        for task_info in all_tasks:
            task_type, subtopic_idx, subtopic_title, subsubtopic_idx, subsubtopic_title = task_info
            target_title = subtopic_title if task_type == "subtopic" else subsubtopic_title
            
            if target_title not in successful_titles:
                logger.warning(f"ç‚ºå¤±æ•—ä»»å‹™ç”Ÿæˆfallback: {target_title}")
                fallback_result = call_model_unified(task_info, raw_analysis, f"fallback_{target_title}")
                if fallback_result:
                    completed_results.append(fallback_result)

        # è™•ç†çµæœï¼ˆä¿®å¾©å­æ¨™é¡Œé‡è¤‡å•é¡Œï¼‰
        grouped_results = {}
        for task_type, subtopic_idx, subtopic_title, subsubtopic_idx, target_title, html_content in completed_results:
            if subtopic_title not in grouped_results:
                grouped_results[subtopic_title] = {"subtopic_content": None, "subsubtopics": []}
            
            if task_type == "subtopic":
                grouped_results[subtopic_title]["subtopic_content"] = html_content
            else:
                grouped_results[subtopic_title]["subsubtopics"].append((subsubtopic_idx, target_title, html_content))

        # æŒ‰åŸå§‹é †åºæ•´ç†åˆ°resultï¼ˆä¿®å¾©æ¨™é¡Œä½ç½®å•é¡Œï¼‰
        for subtopic_idx, subtopic in enumerate(subtopics):
            subtopic_title = subtopic["title"]
            subsubtopics = subtopic.get("subsubtopics", [])
            
            # é—œéµä¿®å¾©ï¼šç¢ºä¿æ¨™é¡Œåœ¨æ­£ç¢ºä½ç½®ï¼Œä½¿ç”¨æ›´ç²¾ç¢ºçš„æ’åºå‰ç¶´
            if subtopic_title in grouped_results:
                prefix = get_heading_prefix(2, subtopic_idx)
                
                if not subsubtopics:
                    # æ²’æœ‰å­å­æ¨™é¡Œçš„æƒ…æ³ï¼šå…ˆæ·»åŠ æ¨™é ­ï¼Œå†æ·»åŠ å…§å®¹
                    subtopic_content = grouped_results[subtopic_title]["subtopic_content"]
                    if subtopic_content:
                        # æª¢æŸ¥å…§å®¹æ˜¯å¦å·²ç¶“åŒ…å«äº†æ­£ç¢ºæ ¼å¼çš„æ¨™é¡Œ
                        expected_header = f"<h2>{prefix} {subtopic_title}</h2>"
                        if expected_header in subtopic_content:
                            # å…§å®¹å·²åŒ…å«æ­£ç¢ºæ¨™é¡Œï¼Œç›´æ¥ä½¿ç”¨ï¼Œä½¿ç”¨_01ç¢ºä¿åœ¨æ­£ç¢ºä½ç½®
                            result[f"_{subtopic_idx:02d}_01_{subtopic_title}_content"] = subtopic_content
                        elif f"<h2>" in subtopic_content:
                            # å…§å®¹åŒ…å«h2æ¨™é¡Œä½†æ ¼å¼ä¸å°ï¼Œéœ€è¦æ›¿æ›æˆ–ç§»é™¤å¤šé¤˜çš„æ¨™é¡Œ
                            import re
                            cleaned_content = re.sub(r'<h2[^>]*>.*?</h2>', '', subtopic_content, flags=re.DOTALL)
                            # ä½¿ç”¨_00ç¢ºä¿æ¨™é ­åœ¨å‰ï¼Œ_01ç¢ºä¿å…§å®¹åœ¨å¾Œ
                            result[f"_{subtopic_idx:02d}_00_{subtopic_title}_header"] = expected_header
                            result[f"_{subtopic_idx:02d}_01_{subtopic_title}_content"] = cleaned_content
                        else:
                            # å…§å®¹ä¸åŒ…å«æ¨™é¡Œï¼Œæ­£å¸¸æ·»åŠ æ¨™é ­å’Œå…§å®¹
                            result[f"_{subtopic_idx:02d}_00_{subtopic_title}_header"] = expected_header
                            result[f"_{subtopic_idx:02d}_01_{subtopic_title}_content"] = subtopic_content
                        logger.info(f"å·²æ•´ç†å­æ¨™é¡Œ: {subtopic_title}")
                else:
                    # æœ‰å­å­æ¨™é¡Œçš„æƒ…æ³ï¼šå…ˆæ·»åŠ å­æ¨™é¡Œæ¨™é ­ï¼Œç„¶å¾Œè™•ç†å­å­æ¨™é¡Œ
                    # ä½¿ç”¨_00ç¢ºä¿å­æ¨™é¡Œæ¨™é ­åœ¨æœ€å‰é¢
                    result[f"_{subtopic_idx:02d}_00_{subtopic_title}_header"] = f"<h2>{prefix} {subtopic_title}</h2>"
                    
                    sorted_subsubtopics = sorted(grouped_results[subtopic_title]["subsubtopics"], key=lambda x: x[0])
                    for subsubtopic_idx, subsubtopic_title, html_content in sorted_subsubtopics:
                        # ç¢ºä¿å­å­æ¨™é¡Œå…§å®¹çš„æ¨™é¡Œå±¤ç´šæ­£ç¢º
                        subprefix = get_heading_prefix(3, subsubtopic_idx)
                        expected_subheader = f"<h3>{subprefix} {subsubtopic_title}</h3>"
                        
                        if expected_subheader in html_content:
                            # å·²åŒ…å«æ­£ç¢ºçš„h3æ¨™é¡Œï¼Œä½¿ç”¨01+subsubtopic_idxç¢ºä¿é †åº
                            result[f"_{subtopic_idx:02d}_{subsubtopic_idx+1:02d}_{subsubtopic_title}"] = html_content
                        elif f"<h3>" in html_content:
                            # åŒ…å«h3ä½†æ ¼å¼å¯èƒ½ä¸å°ï¼Œç§»é™¤å¾Œé‡æ–°æ·»åŠ 
                            import re
                            cleaned_subcontent = re.sub(r'<h3[^>]*>.*?</h3>', '', html_content, flags=re.DOTALL)
                            final_subcontent = expected_subheader + cleaned_subcontent
                            result[f"_{subtopic_idx:02d}_{subsubtopic_idx+1:02d}_{subsubtopic_title}"] = final_subcontent
                        else:
                            # ä¸åŒ…å«h3æ¨™é¡Œï¼Œæ·»åŠ æ¨™é¡Œ
                            final_subcontent = expected_subheader + html_content
                            result[f"_{subtopic_idx:02d}_{subsubtopic_idx+1:02d}_{subsubtopic_title}"] = final_subcontent
                            
                        logger.info(f"å·²æ•´ç†å­å­æ¨™é¡Œ: [{subtopic_title}] -> {subsubtopic_title}")

    # å»ºç«‹æ¨™é¡Œåˆ°keyçš„æ˜ å°„ï¼Œç”¨æ–¼å¿«é€ŸæŸ¥æ‰¾
    title_to_key_mapping = {}
    for key in result.keys():
        parts = key.split("_")
        if len(parts) >= 4:
            title_name = parts[3]
            title_to_key_mapping[title_name] = key
    
    logger.info(f"å¯ç”¨çš„æ¨™é¡Œæ˜ å°„: {list(title_to_key_mapping.keys())}")

    for chart_result in txt2figure_results:
        title_text = chart_result["title_text"]
        img_static = chart_result.get("img_static")
        if not img_static:
            continue

        # è™•ç† img_static çš„ä¸åŒæ ¼å¼ï¼ˆä¿æŒåŸæœ‰é‚è¼¯ï¼‰
        if isinstance(img_static, dict):
            img_bytes = img_static.get("bytes")
            img_b64 = img_static.get("b64")
            if not img_bytes:
                logger.warning(f"åœ–è¡¨æ•¸æ“šä¸å®Œæ•´ï¼Œè·³é: {title_text}")
                continue
            if not img_b64:
                img_b64 = base64.b64encode(img_bytes).decode("utf-8")
        elif isinstance(img_static, str):
            logger.warning(f"æ”¶åˆ°æœªè™•ç†çš„ S3 URLï¼Œè·³é: {img_static}")
            continue
        else:
            img_bytes = img_static
            img_b64 = base64.b64encode(img_bytes).decode("utf-8")

        # è½‰æˆ data-URI
        data_uri = f"data:image/png;base64,{img_b64}"
        img_html = f'<img src="{data_uri}" style="max-width:100%;height:auto;" />'
        chart_id = str(uuid.uuid4().hex[:8])
        
        # æ–°é‚è¼¯ï¼šæ ¹æ“šåœ–è¡¨çš„ä¾†æºè³‡è¨Šå®šä½
        target_key = None
        
        # æ–¹æ³•1ï¼šå¾åœ–è¡¨çš„contextä¸­å°‹æ‰¾å°æ‡‰çš„æ¨™é¡Œ
        if "context" in chart_result:
            context = chart_result["context"]
            logger.info(f"åœ–è¡¨ '{title_text}' çš„context: {context}")
            for title_name, key in title_to_key_mapping.items():
                if title_name in context:
                    target_key = key
                    logger.info(f"æ ¹æ“šcontextå°‡åœ–è¡¨ '{title_text}' æ’å…¥åˆ° '{title_name}'")
                    break
        
        # æ–¹æ³•2ï¼šå¾åœ–è¡¨çš„questionä¸­å°‹æ‰¾å°æ‡‰çš„æ¨™é¡Œ
        if not target_key and "question" in chart_result:
            question = chart_result["question"]
            logger.info(f"åœ–è¡¨ '{title_text}' çš„question: {question}")
            for title_name, key in title_to_key_mapping.items():
                if title_name in question:
                    target_key = key
                    logger.info(f"æ ¹æ“šquestionå°‡åœ–è¡¨ '{title_text}' æ’å…¥åˆ° '{title_name}'")
                    break
        
        # æ–¹æ³•3ï¼šå˜—è©¦å¾title_textæœ¬èº«æ¨æ–·ï¼ˆç§»é™¤å¾Œç¶´å¾Œæ¯”å°ï¼‰
        if not target_key:
            clean_title = title_text.replace("(ç™¼ç¥¨æ•¸æ“š)", "").strip()
            logger.info(f"åœ–è¡¨æ¸…ç†å¾Œçš„æ¨™é¡Œ: {clean_title}")
            for title_name, key in title_to_key_mapping.items():
                if title_name in clean_title or clean_title in title_name:
                    target_key = key
                    logger.info(f"æ ¹æ“šæ¸…ç†å¾Œæ¨™é¡Œå°‡åœ–è¡¨ '{title_text}' æ’å…¥åˆ° '{title_name}'")
                    break
        
        # Fallbackï¼šä½¿ç”¨å›ºå®šå°æ‡‰é—œä¿‚
        if not target_key:
            fig_show_subtitle = ["ä¸»å°å“ç‰ŒéŠ·å”®æ¦‚æ³", "å¹³åƒ¹å¸¶å¸‚å ´æ¦‚æ³", "é«˜åƒ¹å¸¶å¸‚å ´æ¦‚æ³", "åƒ¹æ ¼å¸¶çµæ§‹èˆ‡ç­–ç•¥å®šä½"]
            chart_index = txt2figure_results.index(chart_result)
            target_subtitle = fig_show_subtitle[chart_index % len(fig_show_subtitle)]
            
            for key in result.keys():
                if target_subtitle in key:
                    target_key = key
                    break
            
            if target_key:
                logger.warning(f"ä½¿ç”¨fallbacké‚è¼¯å°‡åœ–è¡¨ '{title_text}' æ’å…¥åˆ° '{target_subtitle}'")
        
        # æœ€å¾Œçš„fallback
        if not target_key:
            content_keys = [k for k in result.keys() if not k.endswith("_header")]
            if content_keys:
                target_key = sorted(content_keys)[0]
                logger.warning(f"æ‰¾ä¸åˆ°åˆé©ä½ç½®ï¼Œå°‡åœ–è¡¨ '{title_text}' æ’å…¥åˆ° '{target_key}'")
            else:
                logger.error(f"ç„¡æ³•æ’å…¥åœ–è¡¨ '{title_text}'ï¼Œè·³éæ­¤åœ–è¡¨")
                continue
        
        if target_key:
            # HTMLåœ–è¡¨è™•ç†
            html_placeholder = create_chart_placeholder(chart_id)
            chart_data[chart_id] = {
                "title_text": title_text,
                "html": img_html,
                "static": img_bytes
            }
            
            # Wordåœ–è¡¨è™•ç†
            word_placeholder = f"[WORD_CHART_{chart_id}]"
            
            # æ’å…¥ä½”ä½ç¬¦
            result[target_key] += "\n" + html_placeholder + "\n"
            result[target_key] += f"\n<div class='word-chart-placeholder'>{word_placeholder}</div>\n"
            
            # ä¿å­˜Wordåœ–è¡¨æ•¸æ“š
            page_name = extract_page_name_from_key(target_key)
            if page_name not in word_chart_data:
                word_chart_data[page_name] = []

            word_chart_entry = {
                "chart_id": chart_id,
                "title_text": title_text,
                "img_static_b64": img_b64,
                "placeholder": word_placeholder,
                "target_section": extract_page_name_from_key(target_key),
                "target_key": target_key
            }
            word_chart_data[page_name].append(word_chart_entry)
            
            logger.info(f"æˆåŠŸæ’å…¥åœ–è¡¨ '{title_text}' åˆ° '{target_key}'")

    return {
        "content": result, 
        "charts": chart_data,
        "word_charts": word_chart_data
    }

def extract_page_name_from_key(key: str) -> str:
    """å¾keyä¸­æå–é é¢åç¨±"""
    # ä¾‹å¦‚: "_01_00_ä¸»å°å“ç‰ŒéŠ·å”®æ¦‚æ³_header" -> "ä¸»å°å“ç‰ŒéŠ·å”®æ¦‚æ³"
    parts = key.split("_")
    if len(parts) >= 4:
        return parts[3]
    return "unknown_page"

def call_model_unified(task_info, raw_analysis, task_id=""):
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„çµ±ä¸€ä»»å‹™è™•ç†å‡½æ•¸"""
    task_type, subtopic_idx, subtopic_title, subsubtopic_idx, subsubtopic_title = task_info
    
    # åŸºæ–¼ä»»å‹™ç¸½ç´¢å¼•é€²è¡ŒéŒ¯é–‹
    if task_type == "subtopic":
        global_task_idx = subtopic_idx
        target_title = subtopic_title
        heading_level = 2
        prefix = get_heading_prefix(2, subtopic_idx)
        # æ–°å¢ï¼šå¾ output_format ç²å–å°æ‡‰çš„ prompt
        subtopic_prompt = get_subtopic_prompt(subtopic_title)
    else:
        global_task_idx = subtopic_idx * 10 + subsubtopic_idx
        target_title = subsubtopic_title
        heading_level = 3
        prefix = get_heading_prefix(3, subsubtopic_idx)
        # æ–°å¢ï¼šå¾ output_format ç²å–å°æ‡‰çš„ prompt
        subtopic_prompt = get_subsubtopic_prompt(subtopic_title, subsubtopic_title)
    
    logger.info(f"è™•ç†ä»»å‹™ [{task_type}] [{subtopic_title}] -> {target_title or '(å­æ¨™é¡Œå…§å®¹)'}")
    
    # æ§‹å»ºsystem_prompt - ç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸å†åŒ…å«å…·é«”ä»»å‹™æè¿°
    if task_type == "subtopic":
        system_prompt = f"""You are a market insight report integration assistant specializing in data structuring and visualization. 
Your task is to transform raw analysis text into a structured JSON containing HTML-formatted report sections.

IMPORTANT: Your output MUST be a valid JSON object with the specified subtopic as key and HTML content as value.

STRICT CONTENT RULES:
- DO NOT fabricate or assume information that is not explicitly present in the provided raw analysis, unless handling a missing subtopic as described below.
- You must extract and restructure content ONLY from the provided raw analysis text for all subtopics that have data. 
- Do NOT extrapolate, infer, or augment the analysis with your own ideas unless explicitly instructed to do so in rule 3 below.
- Do not exceed 1400 words. Do not fall below 1300 words.

PRECISE INSTRUCTIONS:
0. Your response MUST begin with a ```json code block, and contain ONLY the JSON object inside. Do not include any explanation, greeting, or comment before or after the JSON.
1. YOUR RESPONSE MUST BE A VALID JSON OBJECT that matches the example format exactly.
2. For the subtopic that has relevant data:
- Create a key in the JSON with the exact subtopic name.
- The value must be properly formatted HTML that includes:
    - Section heading using `<h2>{prefix} {subtopic_title}</h2>`
    - Content paragraphs using `<p>...</p>` 
    - Lists using `<ul><li>...</li></ul>` for bullet points
    - Important data highlighted with `<strong>` or `<em>` tags
3. For subtopic that is missing information in the provided analysis:
- FIRST, create an HTML block following this structure:
    `<h2>{prefix} {subtopic_title}</h2>`
- THEN, immediately continue by using your own general knowledge to generate a plausible, high-quality analysis for that subtopic in properly structured HTML, without requiring additional user input.
4. your response must be COMPLETE, well-structured, and insightful. You are required to produce between 1300 and 1400 words in total output.
5. your output should be designed to SCORE 3 in each of the following evaluation dimensions: {evaluation_prompt_en}
6. Use only secure HTML (no scripts, iframes, or external resources).
7. Ensure your final output is a properly formatted JSON object that can be parsed without errors.
8. NEVER use raw quotation marks (single or double) inside HTML tags like <strong> or <em>. Escape or move them outside the tag.
9. ç¹é«”ä¸­æ–‡èªæ°£
10. DO NOT generate img HTML or script tags

REQUIRED OUTPUT FORMAT:
```json
{{
"{subtopic_title}": "<h2>{prefix} {subtopic_title}</h2><p>...</p>"
}}
```"""
    else:
        system_prompt = f"""You are a market insight report integration assistant specializing in data structuring and visualization. 
Your task is to transform raw analysis text into a structured JSON containing HTML-formatted report sections.

IMPORTANT: Your output MUST be a valid JSON object with the specified subtopics as keys and HTML content as values.

STRICT CONTENT RULES:
- DO NOT fabricate or assume information that is not explicitly present in the provided raw analysis, unless handling a missing subtopic as described below.
- You must extract and restructure content ONLY from the provided raw analysis text for all subtopics that have data. 
- Do NOT extrapolate, infer, or augment the analysis with your own ideas unless explicitly instructed to do so in rule 3 below.
- Do not exceed 1400 words. Do not fall below 1300 words.

PRECISE INSTRUCTIONS:
0. Your response MUST begin with a ```json code block, and contain ONLY the JSON object inside. Do not include any explanation, greeting, or comment before or after the JSON.
1. YOUR RESPONSE MUST BE A VALID JSON OBJECT that matches the example format exactly.
2. For each subtopic that has relevant data:
- Create a key in the JSON with the exact subtopic name.
- The value must be properly formatted HTML that includes:
    - Section heading using `<h3>{prefix} {subsubtopic_title}</h3>`
    - Content paragraphs using `<p>...</p>` 
    - Lists using `<ul><li>...</li></ul>` for bullet points
    - Important data highlighted with `<strong>` or `<em>` tags
3. For subtopics that are missing information in the provided analysis:
- FIRST, create an HTML block following this structure:
    `<h3>{prefix} {subsubtopic_title}</h3>`
- THEN, immediately continue by using your own general knowledge to generate a plausible, high-quality analysis for that subtopic in properly structured HTML, without requiring additional user input.
4. your response must be COMPLETE, well-structured, and insightful. You are required to produce between 1300 and 1400 words in total output.
5. your output should be designed to SCORE 3 in each of the following evaluation dimensions: {evaluation_prompt_en}
6. Use only secure HTML (no scripts, iframes, or external resources).
7. Ensure your final output is a properly formatted JSON object that can be parsed without errors.
8. NEVER use raw quotation marks (single or double) inside HTML tags like <strong> or <em>. Escape or move them outside the tag.
9. ç¹é«”ä¸­æ–‡æ€ç¶­
10. DO NOT generate img HTML or script tags

REQUIRED OUTPUT FORMAT:
```json
{{
"{subsubtopic_title}": "<h3>{prefix} {subsubtopic_title}</h3><p>...</p>"
}}
```"""
    
    try:
        start_time = time.time()
        # å°‡ subtopic_prompt åŠ å…¥ user input
        raw_text = get_response_invoke(system_prompt, raw_analysis, subtopic_prompt, task_id)
        
        if not raw_text.strip():
            raise Exception("è¿”å›å…§å®¹ç‚ºç©º")
            
        parsed = parse_json_from_text(raw_text)
        html_piece = parsed.get(target_title, "")
        
        if not html_piece.strip():
            raise Exception("è§£æå¾ŒHTMLå…§å®¹ç‚ºç©º")
        
        elapsed = time.time() - start_time
        logger.info(f"ä»»å‹™å®Œæˆ [{task_type}] [{subtopic_title}] -> {target_title or '(å­æ¨™é¡Œå…§å®¹)'} (è€—æ™‚: {elapsed:.2f}s)")
        
        return task_type, subtopic_idx, subtopic_title, subsubtopic_idx, target_title, html_piece
        
    except Exception as err:
        logger.warning(f"âš ï¸ ä»»å‹™è™•ç†å¤±æ•— [{task_type}] [{subtopic_title}] -> {target_title or '(å­æ¨™é¡Œå…§å®¹)'}: {err}")
        
        # ç”Ÿæˆfallbackå…§å®¹
        if task_type == "subtopic":
            fallback = f"""<h2>{prefix} {subtopic_title}</h2>
<div style="background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 8px; margin: 20px 0;">
    <p><strong>âš ï¸ å…§å®¹ç”Ÿæˆä¸­é‡åˆ°æŠ€è¡“å•é¡Œ</strong></p>
    <p>æ­¤éƒ¨åˆ†å…§å®¹æš«æ™‚ç„¡æ³•é¡¯ç¤ºï¼Œç³»çµ±æ­£åœ¨è™•ç†ä¸­ã€‚</p>
</div>"""
        else:
            fallback = f"""<h3>{prefix} {subsubtopic_title}</h3>
<div style="background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 8px; margin: 20px 0;">
    <p><strong>âš ï¸ å…§å®¹ç”Ÿæˆä¸­é‡åˆ°æŠ€è¡“å•é¡Œ</strong></p>
    <p>æ­¤éƒ¨åˆ†å…§å®¹æš«æ™‚ç„¡æ³•é¡¯ç¤ºï¼Œç³»çµ±æ­£åœ¨è™•ç†ä¸­ã€‚</p>
</div>"""
        
        return task_type, subtopic_idx, subtopic_title, subsubtopic_idx, target_title, fallback

def get_response_invoke(system_prompt: str, raw_analysis: str, subtopic_prompt: str, task_id: str = "") -> str:
    # å°‡ subtopic_prompt åŠ å…¥ user input
    user_content = f"åˆ†æä»»å‹™ï¼š{subtopic_prompt}\n\nAI Agent-Extracted Market Insightsï¼š\n{raw_analysis}"
    messages = [{"role": "user", "content": user_content}]
     
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "system": system_prompt,
        "messages": messages,
        "max_tokens": 4096,
        "temperature": 0.3,
    })

    max_retries = 8
    base_wait = 1.0
    
    for attempt in range(max_retries):
        try:
            resp = Connections.bedrock_client.invoke_model(
                body=body,
                modelId=Connections.output_format_fm,
            )
            result = resp["body"].read().decode("utf-8")
            
            # æ–°å¢ï¼šé©—è­‰çµæœæ˜¯å¦åŒ…å«æœ‰æ•ˆå…§å®¹
            if result.strip() and len(result.strip()) > 50:  # åŸºæœ¬å…§å®¹æª¢æŸ¥
                return result
            else:
                logger.warning(f"ä»»å‹™ {task_id} ç¬¬ {attempt+1} æ¬¡å˜—è©¦è¿”å›å…§å®¹éçŸ­")
                if attempt == max_retries - 1:
                    return result  # æœ€å¾Œä¸€æ¬¡å˜—è©¦ä»è¿”å›çµæœ
                    
        except Exception as e:
            error_msg = str(e)
            
            if "Too many tokens" in error_msg or "ThrottlingException" in error_msg:
                # é€€é¿ç­–ç•¥
                wait_time = min(60, base_wait * (2 ** attempt)) + random.uniform(0, 3) + (hash(task_id) % 10) * 0.2
                logger.warning(f"âš ï¸ ä»»å‹™ {task_id} è¢«é™æµï¼Œç¬¬ {attempt+1} æ¬¡é‡è©¦ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                time.sleep(wait_time)
                continue
            elif "Connection" in error_msg:
                wait_time = min(10, 0.5 * (2 ** attempt)) + random.uniform(0, 0.5)
                logger.warning(f"âš ï¸ ä»»å‹™ {task_id} é€£æ¥å•é¡Œï¼Œç¬¬ {attempt+1} æ¬¡é‡è©¦ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                time.sleep(wait_time)
                continue
            else:
                logger.error(f"ä»»å‹™ {task_id} é‡åˆ°å…¶ä»–éŒ¯èª¤: {error_msg}")
                continue
    
    logger.error(f"âŒ ä»»å‹™ {task_id} æœ€å¤šé‡è©¦æ¬¡æ•¸å·²é”ï¼Œæ”¾æ£„")
    return ""

def get_subtopic_prompt(subtopic_title: str) -> str:
    """å¾æ–°çš„ output_format çµæ§‹ç²å–å­æ¨™é¡Œçš„ prompt"""
    for main_topic, main_data in output_format.items():
        for subtopic in main_data.get("subtopics", []):
            if subtopic["title"] == subtopic_title:
                return subtopic.get("prompt", f"è«‹åˆ†æ {subtopic_title} ç›¸é—œå…§å®¹")
    return f"è«‹åˆ†æ {subtopic_title} ç›¸é—œå…§å®¹"

def get_subsubtopic_prompt(subtopic_title: str, subsubtopic_title: str) -> str:
    """å¾æ–°çš„ output_format çµæ§‹ç²å–å­å­æ¨™é¡Œçš„ prompt"""
    for main_topic, main_data in output_format.items():
        for subtopic in main_data.get("subtopics", []):
            if subtopic["title"] == subtopic_title:
                subsubtopics = subtopic.get("subsubtopics", [])
                for subsub in subsubtopics:
                    if isinstance(subsub, dict) and subsub.get("title") == subsubtopic_title:
                        return subsub.get("prompt", f"è«‹åˆ†æ {subsubtopic_title} ç›¸é—œå…§å®¹")
    return f"è«‹åˆ†æ {subsubtopic_title} ç›¸é—œå…§å®¹"

def retry_failed_tasks(failed_tasks, raw_analysis, max_retry_rounds=2):
    """é‡æ–°åŸ·è¡Œå¤±æ•—çš„ä»»å‹™ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥"""
    retry_round = 0
    remaining_failed = failed_tasks.copy()
    successful_results = []
    
    while remaining_failed and retry_round < max_retry_rounds:
        retry_round += 1
        logger.info(f"ğŸ”„ ç¬¬ {retry_round} è¼ªé‡è©¦ï¼Œè™•ç† {len(remaining_failed)} å€‹å¤±æ•—ä»»å‹™")
        
        # ä½¿ç”¨å–®ç·šç¨‹é‡è©¦ï¼Œé¿å…å†æ¬¡é™æµ
        for task_info in remaining_failed:
            try:
                # å¢åŠ ä»»å‹™é–“éš”ï¼Œé¿å…é™æµ
                time.sleep(random.uniform(2, 5))
                
                task_type, subtopic_idx, subtopic_title, subsubtopic_idx, subsubtopic_title = task_info
                
                # ç”Ÿæˆä»»å‹™IDç”¨æ–¼é‡è©¦
                if task_type == "subtopic":
                    target_title = subtopic_title
                    task_id = f"retry_{retry_round}_{subtopic_title}"
                else:
                    target_title = subsubtopic_title
                    task_id = f"retry_{retry_round}_{subsubtopic_title}"
                
                logger.info(f"ğŸ”„ é‡è©¦ä»»å‹™: {task_id}")
                
                # é‡æ–°èª¿ç”¨åŸæœ‰çš„ä»»å‹™è™•ç†é‚è¼¯
                result_tuple = call_model_unified(task_info, raw_analysis, task_id)
                
                if result_tuple and result_tuple[5]:  # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå…§å®¹
                    successful_results.append(result_tuple)
                    logger.info(f"âœ… é‡è©¦æˆåŠŸ: {task_id}")
                else:
                    logger.warning(f"âš ï¸ é‡è©¦ä»å¤±æ•—: {task_id}")
                    
            except Exception as e:
                logger.error(f"âŒ é‡è©¦éç¨‹ä¸­å‡ºéŒ¯: {task_info}, éŒ¯èª¤: {e}")
        
        # æ›´æ–°å¤±æ•—åˆ—è¡¨ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›å¯æ ¹æ“šéœ€è¦å„ªåŒ–ï¼‰
        remaining_failed = []  # ç°¡åŒ–ï¼šä¸å†é‡è¤‡é‡è©¦åŒä¸€ä»»å‹™
        
    return successful_results

def inject_charts_into_html(html_content: str, chart_data: Dict[str, Any]) -> str:
    if not chart_data:
        return html_content

    final_html = html_content   # ä¹‹å‰åŠ çš„ plotly_cdn å¯ä¿ç•™

    for chart_id, data in chart_data.items():
        placeholder_id = f"plotly-placeholder-{chart_id}"
        chart_html = data["html"]

        chart_html_escaped = (
            chart_html.replace("\\", "\\\\")
                      .replace("`", "\\`")
                      .replace("</script>", "<\\/script>")
        )

        inject_script = f"""
        <script>(function() {{
            var ph = document.getElementById("{placeholder_id}");
            if (!ph) return;

            // 1ï¸âƒ£ å»º iframe
            var frame = document.createElement("iframe");
            frame.style.width      = "100%";   // æ»¿ç‰ˆå¯¬
            frame.style.maxWidth   = "600px";  // ä½†æœ€å¤š 600
            frame.style.aspectRatio= "4 / 3";  // 4:3 å¤–è§€ï¼›ç€è¦½å™¨ä¸æ”¯æ´æ™‚å†è£œ height
            frame.style.height     = "auto";
            frame.style.border     = "none";
            frame.style.display    = "block";
            frame.style.margin     = "0 auto"; // ç½®ä¸­

            ph.replaceWith(frame);

            // 2ï¸âƒ£ æŠŠæ•´æ®µ Plotly HTML å¯«é€²å»
            var doc = frame.contentDocument || frame.contentWindow.document;
            doc.open();
            doc.write(`{chart_html_escaped}`);
            doc.close();
        }})();</script>
        """

        final_html += inject_script

    return final_html

# ---------------------------------------------------------------------
# Lambda å…¥å£
# ---------------------------------------------------------------------
def lambda_handler(event: Dict[str, Any], context):
    """
    AWS Lambda Handler â€“ ä¸»æµç¨‹

    1. è®€å–å‰ç«¯å‚³ä¾†çš„ `query`, `session_id`, `topic`, `company_info`
    2. å‘¼å« Bedrock Agent â†’ å–å¾—åˆ†ææ–‡å­—
    3. ç”¨ Claude 4 Sonnet è½‰ JSON+HTML
    4. æ™ºèƒ½æ’å…¥åœ–è¡¨ä¸¦çµ„åˆä¾†æº / HTML å›å‚³çµ¦å‰ç«¯
    """
    logger.info("Event: %s", json.dumps(event, ensure_ascii=False))

    body = event.get("body", {})
    query: str = body.get("query", "")
    session_id: str = body.get("session_id", "")
    topic: str = body.get("session_attributes", {}).get("analysis_topic", "")
    company_info: dict = body.get("session_attributes", {}).get("company_info", {})

    try:
        # 1. Agent
        logger.info("ğŸ¤– èª¿ç”¨ Bedrock Agent...")
        agent_resp = invoke_agent(query, session_id, topic, company_info)
        
        # å‚³é topic åƒæ•¸çµ¦ get_agent_response
        answer_raw, refs, txt2figure_results = get_agent_response(agent_resp, topic)

        logger.info("Agent response: %s", answer_raw)
        logger.info(f"æ‰¾åˆ° {len(txt2figure_results)} å€‹åœ–è¡¨")

        # 2. Format to HTML JSON with smart chart placement
        logger.info("ğŸ“Š æ ¼å¼åŒ–å…§å®¹ä¸¦æ’å…¥åœ–è¡¨...")
        output_data = build_output_format(answer_raw, topic, txt2figure_results)
        
        # 3. çµ„åˆæœ€çµ‚HTML
        html_result = combine_html_from_json(output_data["content"])
        
        # 4. æ³¨å…¥åœ–è¡¨
        if output_data["charts"]:
            logger.info(f"ğŸ’‰ æ³¨å…¥ {len(output_data['charts'])} å€‹åœ–è¡¨...")
            html_result = inject_charts_into_html(html_result, output_data["charts"])

        # 5. è™•ç†ä¾†æº
        logger.info("ğŸ“š è™•ç†åƒè€ƒä¾†æº...")
        src_parts = []
        if refs:
            src_parts.append("<h4>åƒè€ƒä¾†æº</h4>" + source_link(refs))

        # å»é™¤ç©ºå€¼ã€å”¯ä¸€åŒ–ï¼ˆä¿é †åºï¼‰ï¼ŒåŒ…è£æˆ HTML
        src_parts = list(dict.fromkeys(p for p in src_parts if p))
        src_str = "\n\n".join(
            f"<div class='report-source-item'>{part}</div>" for part in src_parts
        )
        logger.info("åƒè€ƒä¾†æºè™•ç†å®Œç•¢: %s", src_str)

        # ç›´æ¥ä½¿ç”¨å·²è™•ç†çš„æ•¸æ“š
        response = {
            "answer": html_result,
            "source": src_str,
            "word_export_data": {
                "content": output_data["content"],
                "charts_data": output_data["word_charts"],  # ç›´æ¥ä½¿ç”¨å·²è™•ç†çš„æ•¸æ“š
                "metadata": {
                    "topic": topic,
                    "company_info": company_info,
                    "session_id": session_id
                }
            }
        }

        total_charts = sum(len(charts) for charts in output_data["word_charts"].values())
        logger.info(f"ğŸ“Š è™•ç†å®Œæˆï¼ŒåŒ…å« {total_charts} å€‹åœ–è¡¨")
        return response
        
    except Exception as err:
        logger.exception("Handler error: %s", err)
        return {"answer": "ç³»çµ±éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "source": ""}