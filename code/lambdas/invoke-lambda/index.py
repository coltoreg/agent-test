"""
invoke-lambda â€“ ä¾ä½¿ç”¨è€…è¼¸å…¥èª¿ç”¨ Bedrock Agentï¼Œä¸¦å°‡å›æ‡‰æ•´ç†æˆ
ç¬¦åˆå‰ç«¯ (Streamlit) ä½¿ç”¨çš„ HTML ç‰‡æ®µã€‚

â€¢ å…ˆä»¥ session_id å€åˆ†åŒä¸€ä½ä½¿ç”¨è€…ä¸€æ¬¡ã€Œå®Œæ•´åˆ†ææµç¨‹ã€ã€‚
â€¢ è§£æ Agent traceï¼Œå–å‡º:
    1) ä¸»å›ç­”æ–‡å­— (chunk)
    2) SQL queryï¼ˆå¦‚æœ‰ ACTION_GROUPï¼‰
    3) KB ä¾†æºåƒè€ƒé€£çµï¼ˆS3 URIï¼‰
â€¢ å†å‘¼å«ç¬¬äºŒå€‹ FMï¼ˆClaude 3 Sonnetï¼‰æŠŠä¸»å›ç­”â†’HTML+JSON æ ¼å¼åŒ–ã€‚
â€¢ é€é `parse_json_from_text()` ä¿è­‰è¼¸å‡ºä¸€å®šèƒ½è¢« json.loads()ã€‚
"""

from __future__ import annotations

import json
import logging
import os
import re
from typing import Any, Dict, List, Tuple
from urllib.parse import unquote
import time
import random
import uuid
import base64
import concurrent.futures

from connections import Connections
from utils import (
    output_format_pt, 
    evaluation_prompt_en, 
    parse_json_from_text, 
    combine_html_from_json,
    get_heading_prefix
)

# ============ Logger ============
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

_CURRENT_OUTPUT_FORMAT: Dict[str, Any] = {}

# ============ ç’°å¢ƒè®Šæ•¸ ============
AGENT_ID: str = os.environ["AGENT_ID"]
REGION_NAME: str = os.environ["REGION_NAME"]

logger.info("Bedrock Agent ID: %s", AGENT_ID)

# ============ AWS é€£ç·š ============
agent_client = Connections.agent_client
agent_runtime_client = Connections.agent_runtime_client
s3_resource = Connections.s3_resource
s3_client_fbmapping = Connections.s3_client_fbmapping

# ---------------------------------------------------------------------
# Agent helpers
# ---------------------------------------------------------------------
def get_highest_agent_version_alias_id(
    list_resp: Dict[str, Any]
) -> str | None:
    """
    å–å¾—ç›®å‰ Agent æ‰€æœ‰åˆ¥å (Aliases) ä¸­ç‰ˆæœ¬è™Ÿæœ€å¤§çš„é‚£ä¸€å€‹ã€‚

    Parameters
    ----------
    list_resp : dict
        `list_agent_aliases` çš„å›å‚³çµæœã€‚

    Returns
    -------
    str | None
        è‹¥æ‰¾ä¸åˆ°å¯ç”¨ aliasï¼Œå›å‚³ Noneã€‚
    """
    highest = -1
    best_alias_id = None
    for alias in list_resp.get("agentAliasSummaries", []):
        rcfg = alias.get("routingConfiguration")
        if rcfg:
            ver = rcfg[0]["agentVersion"]
            if ver.isdigit() and int(ver) > highest:
                highest = int(ver)
                best_alias_id = alias["agentAliasId"]
    return best_alias_id


def build_prompt(user_input: str, topic: str, company_info: Dict[str, str]) -> str:
    """
    å°‡å…¬å¸åŸºæœ¬è³‡æ–™ + Topic + ä½¿ç”¨è€…å•é¡Œ çµ„æˆ Agent Promptã€‚

    ä¾‹ï¼š
    ã€Œå“ç‰Œåç¨±ï¼šå°ç£å•¤é…’ï¼Œå“ç‰Œæ‰€å±¬ç”¢æ¥­ï¼šå•¤é…’ ...ã€‚
      è«‹ä»¥ã€å¸‚å ´æ¦‚æ³èˆ‡è¶¨å‹¢ã€çš„è§’åº¦â€¦ã€
    """
    base = "ï¼Œ".join(f"{k}ï¼š{v}" for k, v in company_info.items() if v.strip())
    return f"""è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿé€²è¡Œåˆ†æï¼š
    1. ä½¿ç”¨ searchinternet å·¥å…·æœå°‹ç›¸é—œçš„æœ€æ–°å¸‚å ´è³‡è¨Šå’Œè¶¨å‹¢
    2. ä½¿ç”¨ querygluetable å·¥å…·æŸ¥è©¢ç›¸é—œæ•¸æ“šè³‡æ–™
    3. ä½¿ç”¨ knowledge_base å·¥å…·ç²å–å°ˆæ¥­çŸ¥è­˜å’ŒèƒŒæ™¯è³‡è¨Š
    å…¬å¸åŸºæœ¬è³‡æ–™ï¼š{base}
    åˆ†æä¸»é¡Œï¼š{topic}
    å…·é«”å•é¡Œï¼š{user_input}
    è«‹åŸºæ–¼ä»¥ä¸Šä¸‰å€‹å·¥å…·ç²å¾—çš„è³‡è¨Šï¼Œå°ã€Œ{topic}ã€é€²è¡Œå…¨é¢ä¸”å°ˆæ¥­çš„åˆ†æã€‚
    """


def invoke_agent(
    user_input: str, session_id: str, topic: str, company_info: Dict[str, str]
):
    """
    å‘¼å« Bedrock Agent ä¸¦å›å‚³ streaming response (generator ç‰©ä»¶)ã€‚
    """
    alias_id = get_highest_agent_version_alias_id(
        agent_client.list_agent_aliases(agentId=AGENT_ID)
    )
    if not alias_id:
        raise RuntimeError("âŒ æ‰¾ä¸åˆ°å¯ç”¨çš„ Agent Alias")

    prompt = build_prompt(user_input, topic, company_info)
    logger.info(f"prompt:{prompt}")
    return agent_runtime_client.invoke_agent(
        agentId=AGENT_ID,
        agentAliasId=alias_id,
        sessionId=session_id,
        enableTrace=True,
        inputText=prompt,
    )

# ---------------------------------------------------------------------
# Trace / å›æ‡‰è§£æ
# ---------------------------------------------------------------------

def get_agent_response(
    streaming_resp: Dict[str, Any],
    topic: str = "",
) -> Tuple[str, List[str], List[Dict[str, Any]]]:
    """
    è§£æ Bedrock Agent Streaming Response
    ------------------------------------------------
    1. å›ç­”æ–‡å­— (full_text)
    2. ä¾†æº URI / æ¨™é¡Œ (sources)
    3. Vanna åœ–è¡¨è³‡æ–™ (txt2figure_results)
    """
    logger.info("get_agent_response â€“ topic=%s", topic)

    if "completion" not in streaming_resp:
        raise ValueError("Invalid response: missing `completion` field")

    traces: List[Dict[str, Any]] = []
    chunks: List[str] = []

    # é€æ¢çµ„è£æ–‡å­— & æ”¶é›† trace
    for event in streaming_resp["completion"]:
        if "trace" in event:
            traces.append(event["trace"])
        elif "chunk" in event:
            chunks.append(event["chunk"]["bytes"].decode("utf-8", "ignore"))

    full_text = "".join(chunks)

    # ==== æ”¶é›†ä¾†æº ====
    sources: List[str] = []
    try:
        sources += extract_source_list_from_kb(traces)
    except Exception as e:
        logger.warning("extract KB refs error: %s", e)

    try:
        sources += extract_source_list_from_perplexity(traces)
    except Exception as e:
        logger.warning("extract web refs error: %s", e)

    # ==== è§£æ Vanna åœ–è¡¨ ====
    try:
        txt2figure_results = extract_txt2figure_result_from_traces(traces)
    except Exception as e:
        logger.warning("extract Athena refs error: %s", e)
        txt2figure_results = []

    # åƒ…åœ¨æŒ‡å®šä¸»é¡Œæ™‚ï¼ŒæŠŠã€ŒæˆåŠŸå–å¾—åœ–æª”ã€çš„æ¨™é¡Œä¹Ÿåˆ—å…¥ä¾†æº
    if topic == "å¸‚å ´æ¦‚æ³èˆ‡è¶¨å‹¢":
        chart_sources = [
            c["title_text"]
            for c in txt2figure_results
            if c.get("img_html") and (
                (isinstance(c["img_html"], dict) and c["img_html"].get("bytes"))
                or not isinstance(c["img_html"], dict)  # str / S3 URL è¦–ç‚ºæˆåŠŸ
            )
        ]
        sources.extend(chart_sources)
        logger.info("added %d chart titles into sources", len(chart_sources))

    return full_text, sources, txt2figure_results

# ---------------------------------------------------------------------
# Athena-Txt2Figure ä¾†æºè™•ç†
# ---------------------------------------------------------------------
def extract_txt2figure_result_from_traces(traces: List[dict]) -> List[Dict[str, Any]]:
    try:
        for trace in traces:
            logger.info("extract_figure_from_traces - trace: %s", trace)

            vanna_result = _extract_vanna_result_from_trace(trace)
            if vanna_result:
                processed_result = _process_vanna_result(vanna_result)
                # å…ˆè£œ suffixï¼Œå†éæ¿¾æ²’æœ‰çœŸæ­£æ¨™é¡Œçš„
                with_suffix  = _add_title_suffix(processed_result)
                filtered_res = _filter_result_valid_title(with_suffix)
                return filtered_res

        return []

    except Exception as err:
        logger.warning("âš ï¸ Extract Athena refs failed: %s", err)
        return []


def _extract_vanna_result_from_trace(trace: dict) -> list | None:
    """
    å¾ traces ä¸­æå– vanna_result_by_titleï¼Œè½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼ã€‚
    æ”¯æ´æ–°çš„å¤šSQLçµæ§‹ã€‚
    """
    obs = (
        trace.get("trace", {})
            .get("orchestrationTrace", {})
            .get("observation", {})
    )

    # åƒ…è™•ç† ACTION_GROUP
    if obs.get("type") != "ACTION_GROUP":
        return None

    ag_out = obs.get("actionGroupInvocationOutput", {})

    # å„ªå…ˆæª¢æŸ¥ sessionAttributes
    session_attrs = ag_out.get("sessionAttributes", {})
    if session_attrs and "vanna_result_by_title" in session_attrs:
        vanna_by_title = session_attrs["vanna_result_by_title"]
        if isinstance(vanna_by_title, dict):
            # å°‡å­—å…¸å€¼è½‰æ›ç‚ºåˆ—è¡¨ï¼Œä¸¦è™•ç†å¯èƒ½çš„å¤šå€‹çµæœ
            result_list = []
            for title, result_data in vanna_by_title.items():
                if isinstance(result_data, list):
                    # å¦‚æœä¸€å€‹ title å°æ‡‰å¤šå€‹çµæœï¼ˆå¤šå€‹ SQLï¼‰
                    result_list.extend(result_data)
                else:
                    # å–®å€‹çµæœ
                    result_list.append(result_data)
            return result_list

    # å›æº¯ text æ¬„ä½
    text_blob = ag_out.get("text", "")
    if not text_blob:
        return None

    try:
        payload = json.loads(text_blob) if isinstance(text_blob, str) else text_blob
    except json.JSONDecodeError:
        return None

    if isinstance(payload, dict) and "vanna_result_by_title" in payload:
        vanna_by_title = payload["vanna_result_by_title"]
        if isinstance(vanna_by_title, dict):
            # å°‡å­—å…¸å€¼è½‰æ›ç‚ºåˆ—è¡¨ï¼Œä¸¦è™•ç†å¯èƒ½çš„å¤šå€‹çµæœ
            result_list = []
            for title, result_data in vanna_by_title.items():
                if isinstance(result_data, list):
                    # å¦‚æœä¸€å€‹ title å°æ‡‰å¤šå€‹çµæœï¼ˆå¤šå€‹ SQLï¼‰
                    result_list.extend(result_data)
                else:
                    # å–®å€‹çµæœ
                    result_list.append(result_data)
            return result_list

    return None

# ======================================== è®€å– vanna å„²å­˜çš„åœ–ç‰‡
def _parse_s3_uri(uri: str) -> Tuple[str, str]:
    """
    å°‡ s3:// æˆ– https://<bucket>.s3.<region>.amazonaws.com/key
    è§£ææˆ (bucket, key)ã€‚ä¸è² è²¬ä¸‹è¼‰ã€‚
    """
    logger.debug("Parsing S3 URI: %s", uri)

    if uri.startswith("s3://"):
        bucket, key = uri.replace("s3://", "", 1).split("/", 1)
    else:
        m = re.match(r"https://([^.]*)\.s3[.-][^/]+/(.+)", uri)
        if not m:
            logger.error("Unsupported S3 URI format: %s", uri)
            raise ValueError(f"Unsupported S3 URI format: {uri}")
        bucket, key = m.group(1), m.group(2)

    logger.debug("Parsed -> bucket=%s, key=%s", bucket, key)
    return bucket, key

def _download_from_s3(bucket: str, key: str) -> bytes:
    response = s3_client_fbmapping.get_object(Bucket=bucket, Key=key)
    ctype = response.get("ContentType", "")
    logger.debug("Content-Type: %s", ctype)

    # åªæ¥å— text/htmlï¼›å…¶ä»–é¡å‹å¯è¦–æƒ…æ³ä¸Ÿè­¦å‘Š/ä¾‹å¤–
    if not ctype.startswith("text/html"):
        logger.warning("Object %s ä¸¦é HTMLï¼ˆContent-Type=%sï¼‰", key, ctype)
    return response["Body"].read()

def _fetch_s3_object_as_bytes(uri: str) -> bytes:
    """
    å°å¤– APIï¼šè¼¸å…¥ S3 URI/URLï¼Œè¿”å› bytesã€‚
    å…§éƒ¨åªåšã€Œè§£æ + ä¸‹è¼‰ã€å…©æ­¥çµ„åˆï¼Œæ–¹ä¾¿ä¹‹å¾Œæ›æˆç­–ç•¥ patternã€‚
    """
    bucket, key = _parse_s3_uri(uri)
    return _download_from_s3(bucket, key)

def _process_vanna_result(vanna_result: List[dict]) -> List[dict]:
    """
    éè¿´æƒæ vanna_resultï¼›é‡åˆ° img_html=S3 URI å°±ä¸‹è¼‰ï¼Œ
    ä¸¦è½‰æˆ {"bytes": ..., "b64": ...}ã€‚
    """

    def _transform(node: Any) -> Any:
        # Dict â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(node, dict):
            new_node: Dict[str, Any] = {}
            for k, v in node.items():
                if k == "img_html" and isinstance(v, str) and v:
                    logger.info("Fetching S3 object for key '%s': %s", k, v)
                    try:
                        raw = _fetch_s3_object_as_bytes(v)
                        new_node[k] = {
                            "bytes": raw,
                            "b64": base64.b64encode(raw).decode()
                        }
                        logger.debug("Successfully converted '%s' (%d bytes)", k, len(raw))
                    except Exception as exc:
                        # ä¸ raiseï¼Œè®“å¾ŒçºŒæµç¨‹ä¸ä¸­æ–·
                        logger.warning("Failed to fetch S3 object: %s", exc)
                        new_node[k] = None
                else:
                    new_node[k] = _transform(v)
            return new_node

        # List â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if isinstance(node, list):
            return [_transform(item) for item in node]

        # Scalar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        return node

    processed = _transform(vanna_result)
    logger.info("Finished converting vanna_result (total items: %d)", len(vanna_result))
    return processed

def _add_title_suffix(result: List[dict], suffix: str = "(ç™¼ç¥¨æ•¸æ“š)") -> List[dict]:
    """
    ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡è³‡è¨Šç”¨æ–¼åœ–è¡¨å®šä½
    """
    for item in result:
        if isinstance(item, dict):
            orig_title = item.get("title_text") or ""
            question = item.get("question", "")
            
            # ä¿ç•™åŸå§‹å•é¡Œä½œç‚ºcontextï¼Œç”¨æ–¼å¾ŒçºŒçš„åœ–è¡¨å®šä½
            if question and "context" not in item:
                item["context"] = question
            
            if not orig_title:
                if question:
                    simplified_title = question[:30] + "..." if len(question) > 30 else question
                    item["title_text"] = f"{simplified_title}{suffix}"
                else:
                    item["title_text"] = suffix
            elif not orig_title.endswith(suffix):
                item["title_text"] = f"{orig_title}{suffix}"

    return result

def _filter_result_valid_title(result: List[dict], suffix: str = "(ç™¼ç¥¨æ•¸æ“š)") -> List[dict]:
    """
    åªä¿ç•™æœ‰ã€ŒçœŸå¯¦ title_textã€çš„åœ–è¡¨  
    - title_text ç‚ºç©º â†’ æ¨æ£„  
    - title_text åªå‰©å¾Œç¶´ (e.g. "(ç™¼ç¥¨æ•¸æ“š)") â†’ æ¨æ£„
    """
    return [
        item for item in result
        if isinstance(item, dict)
        and item.get("title_text")
        and item["title_text"].strip() != suffix
    ]

# ---------------------------------------------------------------------
# Knowledge-Base ä¾†æºè™•ç†
# ---------------------------------------------------------------------
def extract_source_list_from_kb(traces: List[dict]) -> List[str]:
    """
    å¾ KB Trace èƒå– S3 URI listã€‚
    """
    for t in traces:
        obs = (
            t.get("trace", {})
            .get("orchestrationTrace", {})
            .get("observation", {})
        )
        kb_out = obs.get("knowledgeBaseLookupOutput")
        if kb_out:
            return [r["location"]["s3Location"]["uri"] for r in kb_out["retrievedReferences"]]
    return []


def clean_and_dedup_uris(uris: list[str]) -> list[str]:
    seen = set()
    deduped = []
    for u in uris:
        if not u or not isinstance(u, str):
            continue
        norm = unquote(u.strip().lower())
        if norm not in seen:
            seen.add(norm)
            deduped.append(u)
    return deduped


def source_link(uris: list[str]) -> str:
    uris = clean_and_dedup_uris(uris)

    if not uris:
        return "<p><em>ç›®å‰ç„¡å¯ç”¨çš„ä¾†æºè³‡æ–™ã€‚</em></p>"

    links = []
    source_unknow_idx = 1

    for item in uris:
        try:
            item = item.strip() if item else ""
            if not item:
                continue

            # åˆ¤æ–·æ˜¯å¦ç‚º agent question
            if "(ç™¼ç¥¨æ•¸æ“š)" in item:
                # safe_text = html.escape(item)
                safe_text = item
                links.append(f"<li><span class='question-text'>{safe_text}</span></li>")
                continue

            filename = item.rsplit("/", 1)[-1]
            filename = unquote(filename)
            filename = re.sub(r"[_\-]+", " ", filename).strip()
            # safe_item = html.escape(item)
            safe_item = item

            # åˆ¤æ–· s3 é‚„æ˜¯ http
            if item.lower().startswith("s3://"):
                label = f"{filename}ï¼ˆä¾†æºæª”æ¡ˆï¼‰" if filename else "ä¾†æºæª”æ¡ˆ"
            else:
                label = filename or f"æœªå‘½åä¾†æº {source_unknow_idx}"
                if "æœªå‘½åä¾†æº" in label:
                    source_unknow_idx += 1

            # safe_label = html.escape(label)
            safe_label = label
            links.append(f"<li>. <a href='{safe_item}' target='_blank'>{safe_label}</a></li>")

        except Exception as e:
            logger.warning("Failed to parse URI %s: %s", item, e)
            # links.append(f"<li>. <a href='{html.escape(item)}' target='_blank'>æœªçŸ¥ä¾†æº</a></li>")
            links.append(f"<li>. <a href='{item}' target='_blank'>æœªçŸ¥ä¾†æº</a></li>")

    return "<ul>" + "\n".join(links) + "</ul>"

# ---------------------------------------------------------------------
# Web-Search ä¾†æºè™•ç†
# ---------------------------------------------------------------------
def extract_source_list_from_perplexity(traces: List[dict]) -> List[str]:
    """
    å¾ Perplexity Trace ä¸­æ“·å–å¤–éƒ¨ä¾†æºé€£çµã€‚
    å°‹æ‰¾ trace â†’ orchestrationTrace â†’ observation â†’ actionGroupInvocationOutput.textï¼Œ
    è‹¥è©²æ®µè½ç‚º JSON æ ¼å¼ä¸”å«æœ‰ 'response.sources'ï¼Œå³è¦–ç‚º Perplexity çµæœã€‚
    """
    sources = []

    for t in traces:
        obs = (
            t.get("trace", {})
             .get("orchestrationTrace", {})
             .get("observation", {})
        )
        text = obs.get("actionGroupInvocationOutput", {}).get("text", "")

        try:
            payload = json.loads(text)
            if isinstance(payload, dict):
                source_list = payload.get("response", {}).get("sources", [])
                if isinstance(source_list, list):
                    sources.extend(source_list)
        except json.JSONDecodeError:
            continue  # è·³éé JSON æ ¼å¼

    return sources

# ---------------------------------------------------------------------
# Claude Sonnet â€“ ç”¢ç”Ÿ HTML/JSON å ±å‘Š
# ---------------------------------------------------------------------
def create_chart_placeholder(chart_id: str) -> str:
    """å‰µå»ºåœ–è¡¨ä½”ä½ç¬¦ï¼Œé¿å…JSONåºåˆ—åŒ–å•é¡Œ"""
    return f"""
    <div class="chart-body" id="chart-body-{chart_id}" style="
        padding: 20px;
        min-height: 400px;
    ">
        <div id="plotly-placeholder-{chart_id}" style="
            display: flex;
            align-items: center;
            justify-content: center;
            height: 350px;
            color: #666;
            font-size: 16px;
        ">
            ğŸ“Š åœ–è¡¨åŠ è¼‰ä¸­...
        </div>
    </div>
    """


# ------------------------------------------------------------------
# ------------------------------------------------------------------
# ------------------------------------------------------------------
def _build_path_to_key(topic_cfg: Dict[str, Any]) -> Dict[str, str]:
    """
    æŠŠ output_format çš„çµæ§‹è½‰æˆ {å®Œæ•´è·¯å¾‘: section_key}
    ä¾‹ï¼š
      subtopics.0           -> _00_00_ç”¢æ¥­è¦æ¨¡èˆ‡æˆé•·_header
      subtopics.0.subsubtopics.2 -> _00_03_å¹´åº¦éŠ·å”®è®ŠåŒ–
    """
    mapping = {}
    for s_idx, s in enumerate(topic_cfg["subtopics"]):
        sub_path = f"subtopics.{s_idx}"
        mapping[sub_path] = f"_{s_idx:02d}_00_{s['title']}_header"

        for ss_idx, ss in enumerate(s.get("subsubtopics", [])):
            key = f"_{s_idx:02d}_{ss_idx+1:02d}_{ss['title']}"
            mapping[f"{sub_path}.subsubtopics.{ss_idx}"] = key
    return mapping

def build_output_format(
    raw_analysis: str,
    topic: str,
    txt2figure_results: List[Dict[str, Any]],
    company_info: Dict[str, str]
) -> Dict[str, Any]:
    global _CURRENT_OUTPUT_FORMAT

    # ä¾ç…§ company_info è‡ªå‹•é‡æ–°ç”Ÿæˆ
    if (_CURRENT_OUTPUT_FORMAT.get("_meta_company_info") != company_info):
        _CURRENT_OUTPUT_FORMAT = output_format_pt(
            input_company=company_info.get("ä¼æ¥­åç¨±", ""),
            input_brand=company_info.get("å“ç‰Œåç¨±", ""),
            input_product=company_info.get("å•†å“åç¨±", ""),
            input_product_category=company_info.get("å•†å“é¡å‹", "")
        )
        _CURRENT_OUTPUT_FORMAT["_meta_company_info"] = company_info

    # ------------------------------------------------------------------
    # é©—è­‰èˆ‡åŸºæœ¬è®Šæ•¸
    # ------------------------------------------------------------------
    topic_cfg = _CURRENT_OUTPUT_FORMAT.get(topic)
    if not topic_cfg:
        raise ValueError(f"Unsupported topic: {topic}")
    
    result: Dict[str, str] = {}
    charts: Dict[str, Any] = {}
    word_charts: Dict[str, List[Dict[str, Any]]] = {}

    # ä¸»æ¨™é¡Œ
    result["_000_main_title"] = f"<h1>{topic_cfg.get('title', topic)}</h1>"

    # ------------------------------------------------------------------
    # ç”¢ç”Ÿç« ç¯€ HTMLï¼ˆClaudeï¼‰
    # ------------------------------------------------------------------
    result.update(
        _generate_sections_html(
            topic_cfg=topic_cfg,
            raw_analysis=raw_analysis,
        )
    )

    # ç”Ÿæˆæ¨™é¡Œ â†’ section_key çš„æ˜ å°„ï¼Œå¾ŒçºŒæ’åœ–ç”¨
    title_to_key = {k.split("_", 3)[3]: k for k in result}
    path_to_key = _build_path_to_key(topic_cfg)

    # ------------------------------------------------------------------
    # æ’å…¥ Vanna åœ–è¡¨
    # ------------------------------------------------------------------
    for chart in txt2figure_results:
        _insert_chart(chart, result, title_to_key, path_to_key, charts, word_charts)

    return {"content": result, "charts": charts, "word_charts": word_charts}


# ==========================================================================
# build_output_format çš„è¼”åŠ©å‡½å¼ï¼Œä¿æŒåœ¨åŒä¸€æ¨¡çµ„æœ€æ–¹ä¾¿ç¶­è­·
# ==========================================================================
def _generate_sections_html(
    topic_cfg: Dict[str, Any],
    raw_analysis: str,
) -> Dict[str, str]:
    """
    å¤šåŸ·è¡Œç·’å‘¼å« Claudeï¼Œå°‡ output_format çš„ (sub)subtopic
    è½‰æˆ {section_key: html}ã€‚
    """
    sections: Dict[str, str] = {}
    tasks: List[Tuple] = []

    # --------- 1. æ”¶é›†ä»»å‹™ ---------
    for s_idx, s in enumerate(topic_cfg["subtopics"]):
        if not s.get("subsubtopics"):
            tasks.append(("subtopic", s_idx, s["title"], None, None))
        else:
            for ss_idx, ss in enumerate(s["subsubtopics"]):
                ss_title = ss["title"] if isinstance(ss, dict) else ss
                tasks.append(
                    ("subsubtopic", s_idx, s["title"], ss_idx, ss_title)
                )

    # --------- 2. ä½µç™¼å‘¼å« Claude ---------
    completed: List[Tuple] = []
    failed: List[Tuple] = []

    def _worker(task):
        return call_model_unified(task, raw_analysis)

    with concurrent.futures.ThreadPoolExecutor(
        max_workers=min(16, len(tasks))
    ) as exe:
        future_to_task = {exe.submit(_worker, t): t for t in tasks}
        for fut in concurrent.futures.as_completed(future_to_task):
            task = future_to_task[fut]
            try:
                r = fut.result()
                if r and r[5]:
                    completed.append(r)
                else:
                    failed.append(task)
            except Exception:
                failed.append(task)

    # è‹¥ä»æœ‰å¤±æ•—ä»»å‹™ï¼Œå†èµ°ä¸€æ¬¡åŒæ­¥é‡è©¦
    for task in failed:
        try:
            r = call_model_unified(task, raw_analysis)
            if r and r[5]:
                completed.append(r)
        except Exception:
            pass

    # --------- 3. çµ„è£ HTML ---------
    # ä¾èˆŠçš„é‚è¼¯æ•´ç† header / contentï¼Œç¢ºä¿é †åºæ­£ç¢º
    for (
        task_type,
        s_idx,
        s_title,
        ss_idx,
        target_title,
        html,
    ) in completed:
        if task_type == "subtopic":
            key_header = f"_{s_idx:02d}_00_{s_title}_header"
            key_body   = f"_{s_idx:02d}_01_{s_title}_content"
            if key_header not in sections:
                sections[key_header] = f"<h2>{get_heading_prefix(2, s_idx)} {s_title}</h2>"
            sections[key_body] = html
        else:
            parent_header = f"_{s_idx:02d}_00_{s_title}_header"
            if parent_header not in sections:
                sections[parent_header] = f"<h2>{get_heading_prefix(2, s_idx)} {s_title}</h2>"
            key = f"_{s_idx:02d}_{ss_idx+1:02d}_{target_title}"
            if not html.startswith("<h3"):
                html = f"<h3>{get_heading_prefix(3, ss_idx)} {target_title}</h3>" + html
            sections[key] = html

    return sections


def _insert_chart(chart, result, title_to_key, path_to_key, charts, word_charts) -> None:
    """æŠŠå–®ä¸€ chart ä¾è¦å‰‡æ’é€² HTMLï¼Œä¸¦å¡« chart / word_charts dict"""
    chart_id = chart.get("chart_id") or uuid.uuid4().hex[:8]
    title_text = chart.get("title_text", f"åœ–è¡¨-{chart_id}")
    img_html = chart.get("img_html")
    if not img_html:
        return
    
    # ---- å…ˆåˆ¤æ–·æ˜¯ä¸æ˜¯ html ----
    if isinstance(img_html, dict):                # å·²ç¶“è¢« _process_vanna_result ä¸‹è¼‰å›ä¾†
        html_str = img_html.get("bytes", b"").decode("utf-8", "ignore")
        chart_html = html_str                     # ç›´æ¥æŠŠæ•´ä»½ plotly html å¡é€² iframe
        img_bytes = img_html.get("bytes")         # ä¿ç•™çµ¦ Word åŒ¯å‡ºç”¨
        img_b64 = None
    elif isinstance(img_html, str) and img_html.endswith(".html"):
        # S3 é€£çµé‚„æ²’ä¸‹è¼‰ï¼›ç›´æ¥ iframe æŒ‡å‘å¤–éƒ¨æª”
        chart_html = f"<iframe src='{img_html}' style='width:100%;height:100%;border:none;'></iframe>"
        img_bytes = None
        img_b64 = None
    else:
        # çœŸçš„å°±æ˜¯ png/jpg
        img_bytes, img_b64 = _prepare_bytes_b64(img_html)
        chart_html = f"<img src='data:image/png;base64,{img_b64}' style='max-width:100%;height:auto;'/>" \
                     if img_b64 else f"<img src='{img_html}' style='max-width:100%;height:auto;'/>"

    charts[chart_id] = {
        "title_text": title_text,
        "html": chart_html,
        "static": img_bytes or img_html,
    }

    # --- æ±ºå®šæ”¾å“ª ---
    target_key = _find_target_key(chart, title_to_key, path_to_key, result)
    if not target_key:
        logger.error("no place for chart: %s", title_text)
        return

    # --- å¯«ä½”ä½ç¬¦ã€æ”¶è³‡æ–™ ---
    html_plh = create_chart_placeholder(chart_id)
    result[target_key] += (
        f"\n{html_plh}\n<div class='word-chart-placeholder'>[WORD_CHART_{chart_id}]</div>\n"
    )

    page = extract_page_name_from_key(target_key)
    word_charts.setdefault(page, []).append(
        {
            "chart_id": chart_id,
            "title_text": title_text,
            "img_html_b64": img_b64,
            "placeholder": f"[WORD_CHART_{chart_id}]",
            "target_section": page,
            "target_key": target_key,
            "target_path": chart.get("target_path", ""),
        }
    )


# ==========================================================================
# build_output_format æ›´å°çš„å·¥å…·å‡½å¼ï¼ˆåªåšå–®ä¸€ç°¡å–®ä»»å‹™ï¼‰
# ==========================================================================
def _prepare_bytes_b64(img_html):
    """dict/bytes/URL çµ±ä¸€å› (bytes, b64_str | None)"""
    if isinstance(img_html, dict):
        return img_html.get("bytes"), img_html.get("b64")
    if isinstance(img_html, (bytes, bytearray)):
        b = bytes(img_html)
        return b, base64.b64encode(b).decode()
    return None, None  # URL

def _find_target_key(chart, title_to_key, path_to_key, result):
    """ä¾äº”å±¤å„ªå…ˆåºæ‰¾ section key"""

    # 1. ç›´æ¥æ¯”å° target_path
    tp = chart.get("target_path", "")
    if tp and tp in path_to_key:
        return path_to_key[tp]

    # 2 context
    ctx = chart.get("context", "")
    for t, k in title_to_key.items():
        if t in ctx:
            return k

    # 3 question
    q = chart.get("question", "")
    for t, k in title_to_key.items():
        if t in q:
            return k

    # 4 title_text
    tt = chart.get("title_text", "").replace("(ç™¼ç¥¨æ•¸æ“š)", "").strip()
    for t, k in title_to_key.items():
        if tt in t or t in tt:
            return k

    # 5 fallbackï¼šç¬¬ä¸€å€‹é header çš„ key
    for k in sorted(result):
        if not k.endswith("_header"):
            return k
    return None

# ------------------------------------------------------------------
# ------------------------------------------------------------------
# ------------------------------------------------------------------

def extract_page_name_from_key(key: str) -> str:
    """å¾keyä¸­æå–é é¢åç¨±"""
    # ä¾‹å¦‚: "_01_00_ä¸»å°å“ç‰ŒéŠ·å”®æ¦‚æ³_header" -> "ä¸»å°å“ç‰ŒéŠ·å”®æ¦‚æ³"
    parts = key.split("_")
    if len(parts) >= 4:
        return parts[3]
    return "unknown_page"

def call_model_unified(task_info, raw_analysis, task_id=""):
    """å¸¶é‡è©¦æ©Ÿåˆ¶çš„çµ±ä¸€ä»»å‹™è™•ç†å‡½æ•¸"""
    task_type, subtopic_idx, subtopic_title, subsubtopic_idx, subsubtopic_title = task_info
    
    # åŸºæ–¼ä»»å‹™ç¸½ç´¢å¼•é€²è¡ŒéŒ¯é–‹
    if task_type == "subtopic":
        target_title = subtopic_title
        heading_level = 2
        prefix = get_heading_prefix(2, subtopic_idx)
        # å¾ output_format ç²å–å°æ‡‰çš„ prompt
        subtopic_prompt = get_subtopic_prompt(subtopic_title)
    else:
        target_title = subsubtopic_title
        heading_level = 3
        prefix = get_heading_prefix(3, subsubtopic_idx)
        # å¾ output_format ç²å–å°æ‡‰çš„ prompt
        subtopic_prompt = get_subsubtopic_prompt(subtopic_title, subsubtopic_title)
    
    logger.info(f"è™•ç†ä»»å‹™ [{task_type}] [{subtopic_title}] -> {target_title or '(å­æ¨™é¡Œå…§å®¹)'}")
    
    # æ§‹å»ºsystem_prompt - ç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸å†åŒ…å«å…·é«”ä»»å‹™æè¿°
    if task_type == "subtopic":
        system_prompt = f"""You are a market insight report integration assistant specializing in data structuring and visualization. 
Your task is to transform raw analysis text into a structured JSON containing HTML-formatted report sections.

IMPORTANT: Your output MUST be a valid JSON object with the specified subtopic as key and HTML content as value.

STRICT CONTENT RULES:
- DO NOT fabricate or assume information that is not explicitly present in the provided raw analysis, unless handling a missing subtopic as described below.
- You must extract and restructure content ONLY from the provided raw analysis text for all subtopics that have data. 
- Do NOT extrapolate, infer, or augment the analysis with your own ideas unless explicitly instructed to do so in rule 3 below.
- Do not exceed 1400 words. Do not fall below 1300 words.

PRECISE INSTRUCTIONS:
0. Your response MUST begin with a ```json code block, and contain ONLY the JSON object inside. Do not include any explanation, greeting, or comment before or after the JSON.
1. YOUR RESPONSE MUST BE A VALID JSON OBJECT that matches the example format exactly.
2. For the subtopic that has relevant data:
- Create a key in the JSON with the exact subtopic name.
- The value must be properly formatted HTML that includes:
    - Section heading using `<h2>{prefix} {subtopic_title}</h2>`
    - Content paragraphs using `<p>...</p>` 
    - Lists using `<ul><li>...</li></ul>` for bullet points
    - Important data highlighted with `<strong>` or `<em>` tags
3. For subtopic that is missing information in the provided analysis:
- FIRST, create an HTML block following this structure:
    `<h2>{prefix} {subtopic_title}</h2>`
- THEN, immediately continue by using your own general knowledge to generate a plausible, high-quality analysis for that subtopic in properly structured HTML, without requiring additional user input.
4. your response must be COMPLETE, well-structured, and insightful. You are required to produce between 1300 and 1400 words in total output.
5. your output should be designed to SCORE 3 in each of the following evaluation dimensions: {evaluation_prompt_en}
6. Use only secure HTML (no scripts, iframes, or external resources).
7. Ensure your final output is a properly formatted JSON object that can be parsed without errors.
8. NEVER use raw quotation marks (single or double) inside HTML tags like <strong> or <em>. Escape or move them outside the tag.
9. ç¹é«”ä¸­æ–‡èªæ°£
10. DO NOT generate img HTML or script tags

REQUIRED OUTPUT FORMAT:
```json
{{
"{subtopic_title}": "<h2>{prefix} {subtopic_title}</h2><p>...</p>"
}}
```"""
    else:
        system_prompt = f"""You are a market insight report integration assistant specializing in data structuring and visualization. 
Your task is to transform raw analysis text into a structured JSON containing HTML-formatted report sections.

IMPORTANT: Your output MUST be a valid JSON object with the specified subtopics as keys and HTML content as values.

STRICT CONTENT RULES:
- DO NOT fabricate or assume information that is not explicitly present in the provided raw analysis, unless handling a missing subtopic as described below.
- You must extract and restructure content ONLY from the provided raw analysis text for all subtopics that have data. 
- Do NOT extrapolate, infer, or augment the analysis with your own ideas unless explicitly instructed to do so in rule 3 below.
- Do not exceed 1400 words. Do not fall below 1300 words.

PRECISE INSTRUCTIONS:
0. Your response MUST begin with a ```json code block, and contain ONLY the JSON object inside. Do not include any explanation, greeting, or comment before or after the JSON.
1. YOUR RESPONSE MUST BE A VALID JSON OBJECT that matches the example format exactly.
2. For each subtopic that has relevant data:
- Create a key in the JSON with the exact subtopic name.
- The value must be properly formatted HTML that includes:
    - Section heading using `<h3>{prefix} {subsubtopic_title}</h3>`
    - Content paragraphs using `<p>...</p>` 
    - Lists using `<ul><li>...</li></ul>` for bullet points
    - Important data highlighted with `<strong>` or `<em>` tags
3. For subtopics that are missing information in the provided analysis:
- FIRST, create an HTML block following this structure:
    `<h3>{prefix} {subsubtopic_title}</h3>`
- THEN, immediately continue by using your own general knowledge to generate a plausible, high-quality analysis for that subtopic in properly structured HTML, without requiring additional user input.
4. your response must be COMPLETE, well-structured, and insightful. You are required to produce between 1300 and 1400 words in total output.
5. your output should be designed to SCORE 3 in each of the following evaluation dimensions: {evaluation_prompt_en}
6. Use only secure HTML (no scripts, iframes, or external resources).
7. Ensure your final output is a properly formatted JSON object that can be parsed without errors.
8. NEVER use raw quotation marks (single or double) inside HTML tags like <strong> or <em>. Escape or move them outside the tag.
9. ç¹é«”ä¸­æ–‡æ€ç¶­
10. DO NOT generate img HTML or script tags

REQUIRED OUTPUT FORMAT:
```json
{{
"{subsubtopic_title}": "<h3>{prefix} {subsubtopic_title}</h3><p>...</p>"
}}
```"""
    
    try:
        start_time = time.time()
        # å°‡ subtopic_prompt åŠ å…¥ user input
        raw_text = get_response_invoke(system_prompt, raw_analysis, subtopic_prompt, task_id)
        
        if not raw_text.strip():
            raise Exception("è¿”å›å…§å®¹ç‚ºç©º")
            
        parsed = parse_json_from_text(raw_text)
        html_piece = parsed.get(target_title, "")
        
        if not html_piece.strip():
            raise Exception("è§£æå¾ŒHTMLå…§å®¹ç‚ºç©º")
        
        elapsed = time.time() - start_time
        logger.info(f"ä»»å‹™å®Œæˆ [{task_type}] [{subtopic_title}] -> {target_title or '(å­æ¨™é¡Œå…§å®¹)'} (è€—æ™‚: {elapsed:.2f}s)")
        
        return task_type, subtopic_idx, subtopic_title, subsubtopic_idx, target_title, html_piece
        
    except Exception as err:
        logger.warning(f"âš ï¸ ä»»å‹™è™•ç†å¤±æ•— [{task_type}] [{subtopic_title}] -> {target_title or '(å­æ¨™é¡Œå…§å®¹)'}: {err}")
        
        # ç”Ÿæˆfallbackå…§å®¹
        if task_type == "subtopic":
            fallback = f"""<h2>{prefix} {subtopic_title}</h2>
<div style="background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 8px; margin: 20px 0;">
    <p><strong>âš ï¸ å…§å®¹ç”Ÿæˆä¸­é‡åˆ°æŠ€è¡“å•é¡Œ</strong></p>
    <p>æ­¤éƒ¨åˆ†å…§å®¹æš«æ™‚ç„¡æ³•é¡¯ç¤ºï¼Œç³»çµ±æ­£åœ¨è™•ç†ä¸­ã€‚</p>
</div>"""
        else:
            fallback = f"""<h3>{prefix} {subsubtopic_title}</h3>
<div style="background: #fff3cd; border: 1px solid #ffeaa7; padding: 15px; border-radius: 8px; margin: 20px 0;">
    <p><strong>âš ï¸ å…§å®¹ç”Ÿæˆä¸­é‡åˆ°æŠ€è¡“å•é¡Œ</strong></p>
    <p>æ­¤éƒ¨åˆ†å…§å®¹æš«æ™‚ç„¡æ³•é¡¯ç¤ºï¼Œç³»çµ±æ­£åœ¨è™•ç†ä¸­ã€‚</p>
</div>"""
        
        return task_type, subtopic_idx, subtopic_title, subsubtopic_idx, target_title, fallback

def get_response_invoke(system_prompt: str, raw_analysis: str, subtopic_prompt: str, task_id: str = "") -> str:
    # å°‡ subtopic_prompt åŠ å…¥ user input
    user_content = f"åˆ†æä»»å‹™ï¼š{subtopic_prompt}\n\nAI Agent-Extracted Market Insightsï¼š\n{raw_analysis}"
    messages = [{"role": "user", "content": user_content}]
     
    body = json.dumps({
        "anthropic_version": "bedrock-2023-05-31",
        "system": system_prompt,
        "messages": messages,
        "max_tokens": 4096,
        "temperature": 0.3,
    })

    max_retries = 8
    base_wait = 1.0
    
    for attempt in range(max_retries):
        try:
            resp = Connections.bedrock_client.invoke_model(
                body=body,
                modelId=Connections.output_format_fm,
            )
            result = resp["body"].read().decode("utf-8")
            
            # æ–°å¢ï¼šé©—è­‰çµæœæ˜¯å¦åŒ…å«æœ‰æ•ˆå…§å®¹
            if result.strip() and len(result.strip()) > 50:  # åŸºæœ¬å…§å®¹æª¢æŸ¥
                return result
            else:
                logger.warning(f"ä»»å‹™ {task_id} ç¬¬ {attempt+1} æ¬¡å˜—è©¦è¿”å›å…§å®¹éçŸ­")
                if attempt == max_retries - 1:
                    return result  # æœ€å¾Œä¸€æ¬¡å˜—è©¦ä»è¿”å›çµæœ
                    
        except Exception as e:
            error_msg = str(e)
            
            if "Too many tokens" in error_msg or "ThrottlingException" in error_msg:
                # é€€é¿ç­–ç•¥
                wait_time = min(60, base_wait * (2 ** attempt)) + random.uniform(0, 3) + (hash(task_id) % 10) * 0.2
                logger.warning(f"âš ï¸ ä»»å‹™ {task_id} è¢«é™æµï¼Œç¬¬ {attempt+1} æ¬¡é‡è©¦ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                time.sleep(wait_time)
                continue
            elif "Connection" in error_msg:
                wait_time = min(10, 0.5 * (2 ** attempt)) + random.uniform(0, 0.5)
                logger.warning(f"âš ï¸ ä»»å‹™ {task_id} é€£æ¥å•é¡Œï¼Œç¬¬ {attempt+1} æ¬¡é‡è©¦ï¼Œç­‰å¾… {wait_time:.2f} ç§’")
                time.sleep(wait_time)
                continue
            else:
                logger.error(f"ä»»å‹™ {task_id} é‡åˆ°å…¶ä»–éŒ¯èª¤: {error_msg}")
                continue
    
    logger.error(f"âŒ ä»»å‹™ {task_id} æœ€å¤šé‡è©¦æ¬¡æ•¸å·²é”ï¼Œæ”¾æ£„")
    return ""

def get_subtopic_prompt(subtopic_title: str) -> str:
    # ç›´æ¥å¾å¿«å–æ‹¿
    for main_topic, main_data in _CURRENT_OUTPUT_FORMAT.items():
        for subtopic in main_data.get("subtopics", []):
            if subtopic["title"] == subtopic_title:
                return subtopic.get("prompt", f"è«‹åˆ†æ {subtopic_title} ç›¸é—œå…§å®¹")
    return f"è«‹åˆ†æ {subtopic_title} ç›¸é—œå…§å®¹"

def get_subsubtopic_prompt(subtopic_title: str, subsubtopic_title: str) -> str:
    for main_topic, main_data in _CURRENT_OUTPUT_FORMAT.items():
        for subtopic in main_data.get("subtopics", []):
            if subtopic["title"] == subtopic_title:
                for subsub in subtopic.get("subsubtopics", []):
                    if isinstance(subsub, dict) and subsub.get("title") == subsubtopic_title:
                        return subsub.get("prompt", f"è«‹åˆ†æ {subsubtopic_title} ç›¸é—œå…§å®¹")
    return f"è«‹åˆ†æ {subsubtopic_title} ç›¸é—œå…§å®¹"

def retry_failed_tasks(failed_tasks, raw_analysis, max_retry_rounds=2):
    """é‡æ–°åŸ·è¡Œå¤±æ•—çš„ä»»å‹™ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„ç­–ç•¥"""
    retry_round = 0
    remaining_failed = failed_tasks.copy()
    successful_results = []
    
    while remaining_failed and retry_round < max_retry_rounds:
        retry_round += 1
        logger.info(f"ğŸ”„ ç¬¬ {retry_round} è¼ªé‡è©¦ï¼Œè™•ç† {len(remaining_failed)} å€‹å¤±æ•—ä»»å‹™")
        
        # ä½¿ç”¨å–®ç·šç¨‹é‡è©¦ï¼Œé¿å…å†æ¬¡é™æµ
        for task_info in remaining_failed:
            try:
                # å¢åŠ ä»»å‹™é–“éš”ï¼Œé¿å…é™æµ
                time.sleep(random.uniform(2, 5))
                
                task_type, subtopic_idx, subtopic_title, subsubtopic_idx, subsubtopic_title = task_info
                
                # ç”Ÿæˆä»»å‹™IDç”¨æ–¼é‡è©¦
                if task_type == "subtopic":
                    target_title = subtopic_title
                    task_id = f"retry_{retry_round}_{subtopic_title}"
                else:
                    target_title = subsubtopic_title
                    task_id = f"retry_{retry_round}_{subsubtopic_title}"
                
                logger.info(f"ğŸ”„ é‡è©¦ä»»å‹™: {task_id}")
                
                # é‡æ–°èª¿ç”¨åŸæœ‰çš„ä»»å‹™è™•ç†é‚è¼¯
                result_tuple = call_model_unified(task_info, raw_analysis, task_id)
                
                if result_tuple and result_tuple[5]:  # æª¢æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆå…§å®¹
                    successful_results.append(result_tuple)
                    logger.info(f"âœ… é‡è©¦æˆåŠŸ: {task_id}")
                else:
                    logger.warning(f"âš ï¸ é‡è©¦ä»å¤±æ•—: {task_id}")
                    
            except Exception as e:
                logger.error(f"âŒ é‡è©¦éç¨‹ä¸­å‡ºéŒ¯: {task_info}, éŒ¯èª¤: {e}")
        
        # æ›´æ–°å¤±æ•—åˆ—è¡¨ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯¦éš›å¯æ ¹æ“šéœ€è¦å„ªåŒ–ï¼‰
        remaining_failed = []  # ç°¡åŒ–ï¼šä¸å†é‡è¤‡é‡è©¦åŒä¸€ä»»å‹™
        
    return successful_results

def inject_charts_into_html(html_content: str, chart_data: Dict[str, Any]) -> str:
    if not chart_data:
        return html_content

    final_html = html_content   # ä¹‹å‰åŠ çš„ plotly_cdn å¯ä¿ç•™

    for chart_id, data in chart_data.items():
        placeholder_id = f"plotly-placeholder-{chart_id}"
        chart_html = data["html"]

        chart_html_escaped = (
            chart_html.replace("\\", "\\\\")
                      .replace("`", "\\`")
                      .replace("</script>", "<\\/script>")
        )

        inject_script = f"""
        <script>(function() {{
            var ph = document.getElementById("{placeholder_id}");
            if (!ph) return;

            // 1ï¸âƒ£ å»º iframe
            var frame = document.createElement("iframe");
            frame.style.width      = "100%";   // æ»¿ç‰ˆå¯¬
            frame.style.maxWidth   = "600px";  // ä½†æœ€å¤š 600
            frame.style.aspectRatio= "4 / 3";  // 4:3 å¤–è§€ï¼›ç€è¦½å™¨ä¸æ”¯æ´æ™‚å†è£œ height
            frame.style.height     = "auto";
            frame.style.border     = "none";
            frame.style.display    = "block";
            frame.style.margin     = "0 auto"; // ç½®ä¸­

            ph.replaceWith(frame);

            // 2ï¸âƒ£ æŠŠæ•´æ®µ Plotly HTML å¯«é€²å»
            var doc = frame.contentDocument || frame.contentWindow.document;
            doc.open();
            doc.write(`{chart_html_escaped}`);
            doc.close();
        }})();</script>
        """

        final_html += inject_script

    return final_html

# ---------------------------------------------------------------------
# Lambda å…¥å£
# ---------------------------------------------------------------------
def lambda_handler(event: Dict[str, Any], context):
    """
    AWS Lambda Handler â€“ ä¸»æµç¨‹

    1. è®€å–å‰ç«¯å‚³ä¾†çš„ `query`, `session_id`, `topic`, `company_info`
    2. å‘¼å« Bedrock Agent â†’ å–å¾—åˆ†ææ–‡å­—
    3. ç”¨ Claude 4 Sonnet è½‰ JSON+HTML
    4. æ™ºèƒ½æ’å…¥åœ–è¡¨ä¸¦çµ„åˆä¾†æº / HTML å›å‚³çµ¦å‰ç«¯
    """
    logger.info("Event: %s", json.dumps(event, ensure_ascii=False))

    body = event.get("body", {})
    query: str = body.get("query", "")
    session_id: str = body.get("session_id", "")
    topic: str = body.get("session_attributes", {}).get("analysis_topic", "")
    company_info: dict = body.get("session_attributes", {}).get("company_info", {})

    try:
        # 1. Agent
        logger.info("ğŸ¤– èª¿ç”¨ Bedrock Agent...")
        agent_resp = invoke_agent(query, session_id, topic, company_info)
        
        # å‚³é topic åƒæ•¸çµ¦ get_agent_response
        answer_raw, refs, txt2figure_results = get_agent_response(agent_resp, topic)

        logger.debug("Agent response: %s", answer_raw)
        logger.info(f"æ‰¾åˆ° {len(txt2figure_results)} å€‹åœ–è¡¨")

        # 2. Format to HTML JSON with smart chart placement
        logger.info("ğŸ“Š æ ¼å¼åŒ–å…§å®¹ä¸¦æ’å…¥åœ–è¡¨...")
        output_data = build_output_format(answer_raw, topic, txt2figure_results, company_info)
        
        # 3. çµ„åˆæœ€çµ‚HTML
        html_result = combine_html_from_json(output_data["content"])
        
        # 4. æ³¨å…¥åœ–è¡¨
        if output_data["charts"]:
            logger.info(f"ğŸ’‰ æ³¨å…¥ {len(output_data['charts'])} å€‹åœ–è¡¨...")
            html_result = inject_charts_into_html(html_result, output_data["charts"])

        # 5. è™•ç†ä¾†æº
        logger.info("ğŸ“š è™•ç†åƒè€ƒä¾†æº...")
        src_parts = []
        if refs:
            src_parts.append("<h4>åƒè€ƒä¾†æº</h4>" + source_link(refs))

        # å»é™¤ç©ºå€¼ã€å”¯ä¸€åŒ–ï¼ˆä¿é †åºï¼‰ï¼ŒåŒ…è£æˆ HTML
        src_parts = list(dict.fromkeys(p for p in src_parts if p))
        src_str = "\n\n".join(
            f"<div class='report-source-item'>{part}</div>" for part in src_parts
        )
        logger.info("åƒè€ƒä¾†æºè™•ç†å®Œç•¢: %s", src_str)

        # ç›´æ¥ä½¿ç”¨å·²è™•ç†çš„æ•¸æ“š
        response = {
            "answer": html_result,
            "source": src_str,
            "word_export_data": {
                "content": output_data["content"],
                "charts_data": output_data["word_charts"],  # ç›´æ¥ä½¿ç”¨å·²è™•ç†çš„æ•¸æ“š
                "metadata": {
                    "topic": topic,
                    "company_info": company_info,
                    "session_id": session_id
                }
            }
        }

        total_charts = sum(len(charts) for charts in output_data["word_charts"].values())
        logger.info(f"ğŸ“Š è™•ç†å®Œæˆï¼ŒåŒ…å« {total_charts} å€‹åœ–è¡¨")
        return response
        
    except Exception as err:
        logger.exception("Handler error: %s", err)
        return {"answer": "ç³»çµ±éŒ¯èª¤ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "source": ""}